{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a2afa8",
   "metadata": {},
   "source": [
    "Première étape on installe les programmes nécéssaires à l'entrainement de l'IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e873265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines3 in /home/cafekrem/.local/lib/python3.10/site-packages (2.0.0a10)\n",
      "Requirement already satisfied: cloudpickle in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (2.2.0)\n",
      "Requirement already satisfied: matplotlib in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (3.5.2)\n",
      "Requirement already satisfied: torch>=1.11 in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (2.0.1)\n",
      "Requirement already satisfied: pandas in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (1.22.4)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /home/cafekrem/.local/lib/python3.10/site-packages (from stable-baselines3) (0.28.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/cafekrem/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3) (4.6.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3) (1.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (10.9.0.58)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (2.0.0)\n",
      "Requirement already satisfied: filelock in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (3.12.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.7.101)\n",
      "Requirement already satisfied: networkx in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11->stable-baselines3) (3.0.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.7.99)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (2.14.3)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=1.11->stable-baselines3) (1.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/cafekrem/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->stable-baselines3) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11->stable-baselines3) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/cafekrem/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->stable-baselines3) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/cafekrem/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11->stable-baselines3) (16.0.5.post0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib->stable-baselines3) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib->stable-baselines3) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib->stable-baselines3) (9.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->stable-baselines3) (2022.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym in /home/cafekrem/.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from gym) (1.22.4)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/cafekrem/.local/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from gym) (2.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/cafekrem/.local/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/cafekrem/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3\n",
    "!pip install gym\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52553c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# On import la librairie gymnasium qu'on renome gym dans le contexte d'execution du fichier\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "# Ici on import de la librairie stable_baselines3 l'object DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a036c",
   "metadata": {},
   "source": [
    "## Environement \n",
    "\n",
    "L'environement est élément avec lequel l'IA va intéragir.\n",
    "Cette environement peut représenter un jeu vidéo, une simulation de déplacement de piéton.\n",
    "\n",
    "Durant ce stage, nous allons utiliser la librairie **gym**. Qui met à disposition à un ensemble d'environement sur lequels nous pouvons entrainer une IA. \n",
    "\n",
    "https://www.gymlibrary.dev/environments/atari/\n",
    "https://www.gymlibrary.dev/environments/mujoco/\n",
    "\n",
    "### CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c93f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fba2c7",
   "metadata": {},
   "source": [
    "Ici on instancie l'environement, il s'agit de l'environement suivant.\n",
    "https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "Qui représente un pendule est l'objectif est de ne pas faire tomber le baton.\n",
    "**render_mode** est un argument permetant d'afficher l'environement au sens graphique du terme.\n",
    "\n",
    "le lien ci dessous définit l'ensemble des méthodes de l'environement\n",
    "\n",
    "https://www.gymlibrary.dev/api/core/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8966dc9e",
   "metadata": {},
   "source": [
    "### Espace d'action \n",
    "\n",
    "L'environement définit un espace d'action, ça corresponds au action que nous pouvons faire pour interagir avec l'environement.\n",
    "\n",
    "Si on étais sur mario, cela correspondrais au touche de la manette.\n",
    "\n",
    " **action_space** nous définit le type d'action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fae0661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e35dd3",
   "metadata": {},
   "source": [
    "Ici on nous que l'espace d'action est discret, l'opposé de continue ( valeur à virgule, par example la pédale d'accélération d'une voiture )\n",
    "\n",
    "Dans notre cas l'IA peut faire deux choix à tous instant :\n",
    "- aller à droite\n",
    "- aller à gauche "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea354d0a",
   "metadata": {},
   "source": [
    "### Espace d'observation\n",
    "\n",
    "L'espace d'observation, nous donne l'état de l'environement.  \n",
    "\n",
    "Par example avec Mario l'espace d'observation pourrais être l'image du jeu. Grâce à cette image nous savons quand est-ce qu'il faut faire sauter mario ou non. \n",
    "\n",
    "Dans **observation_space**, nous donne les spécification de l'état, à quoi ressemble l'état. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3fb33bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425bd3a",
   "metadata": {},
   "source": [
    "Ici ça parrais complexe à lire, voir moche. \n",
    "\n",
    "C'est pouyr ça que nous allons regarder la documentation, qui est un plus belle.\n",
    "https://www.gymlibrary.dev/environments/classic_control/cart_pole/#observation-space\n",
    "\n",
    "Nous avons donc 4 informations étant des valeur continue :\n",
    "- *cart position* : la position du cart\n",
    "- *Cart Velocity* : la vitesse du cart\n",
    "- *Pole Angle* : l'angle du baton\n",
    "- *Pole Angular Velocity* : la vitesse de chute du baton\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad77c9",
   "metadata": {},
   "source": [
    "\n",
    "#### Donc à partir de ces informations Quelle action prendre Sébastien ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8680dcf",
   "metadata": {},
   "source": [
    "écrit ta réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b0a58",
   "metadata": {},
   "source": [
    "Tu n'en sait rien et bien moi non plus. Et je n'y connais rien en physique. On va donc \"demander\" à l'IA de faire ce choix pour nous. Pour ça on doit définir l'objectif de notre système"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15457683",
   "metadata": {},
   "source": [
    "### Récompense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6db5eb",
   "metadata": {},
   "source": [
    " On peut vulgariser l'entrainement d'une IA à l'aide de la boite de skinner \n",
    " \n",
    " !!! ATTENTION C'EST UNE VULGARISATION !!!!\n",
    " \n",
    " https://fr.wikipedia.org/wiki/Bo%C3%AEte_de_Skinner\n",
    " \n",
    " Vidéo de 3 minutes de la boite de skinner \n",
    " \n",
    " https://www.youtube.com/watch?v=TuT2358C2bU\n",
    " \n",
    " Ainsi on va dirriger le comportement de notre IA en maximisant la fonction récompense. \n",
    " \n",
    " Donc à chaque choix de l'IA l'environement va soit:\n",
    " - le récompenser\n",
    " - le punir\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7abe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f799b9",
   "metadata": {},
   "source": [
    "Ici la récompense est continue et varie de - l'infinie à + l'infinie.\n",
    "\n",
    "Si on regarde la documentation à la récompense est 1 si le baton n'est pas tombé sinon 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c48c70",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "\n",
    "Afin de déterminer l'action à prendre en fonction de l'état qui maximise la récompense. \n",
    "\n",
    "Pendant ton stage tu va utiliser des solutions de Deep Reinforcement Learning. Tu utiliseras une librairie python **stable_baseline3** qui stocke un ensemble d'algorithm de **deep reinforcement learning**, qui te permettras d'utiliser ces algorithmes comme des boites noires ( au sens où tu ne connais pas le comportement de ces boites )\n",
    "\n",
    "**Deep** veut dire qu'on utilise des réseaux de neuronnes.\n",
    "\n",
    "**Reinforcement Learning** veut dire  qu'on utilise des méthode d'apprentissage basé des récompense cumulative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6551d41",
   "metadata": {},
   "source": [
    "### DQN : Deep Q Network\n",
    "\n",
    "Pendant ton stage tu utiliseras l'algorithm DQN pour Deep Q Network. Au vu du nom ça contiens un réseau de neuronnes. Il reste une lettre à définir le Q.\n",
    "\n",
    "#### Q learning\n",
    "\n",
    "Pour nous allons voir le grand père de cette algorithm le Q learning\n",
    "\n",
    "Voici d'abbord quelques références : \n",
    "- https://www.datacamp.com/tutorial/introduction-q-learning-beginner-tutorial\n",
    "\n",
    "\n",
    "##### Q Table \n",
    "\n",
    "le Q learning ce construit sur une Q table. C'est une tableau à deux dimension. Chaque collones corresponds à une action, chaque ligne à un état. La valeur d'un cases action, état corresponds à l'espérance de récompense pour ce couple. \n",
    "\n",
    "Ainsi le principe du Q learning est de tester beaucoup de combinaison d'état,action afin de remplir ce tableau. Lorsque l'IA devras choisir une action çà l'aide du Q-learning il prendras l'action qui à la plus grande espérance de récompense pour l'état courrant. \n",
    "\n",
    "##### Limites\n",
    "\n",
    "Cependant lorsque l'environement définie un espace d'observation continue ( un angle, une distance ... ). Il faut convertir ces valeur continue en valeur discrète et donc perdre de la précision et avoir un Q table de plus en plus en fonction de la précision voulue. \n",
    "\n",
    "####  DQN \n",
    "\n",
    "Pour pallier à ce problème, on utilise des réseaux de neuronnes permettant d'accepter des valeurs et d'obtenir le compromis forte précision et espace mémoire.\n",
    "\n",
    "Pour sélectionner la meilleur action, le principe reste le même. Le réseau de neuronnes prends en paramètres l'état de l'environement et renvoie pour chaque action l'espérance de récompense. Et donc l'agent choisie l'action avec la plus haute espérance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2aefb3",
   "metadata": {},
   "source": [
    "## Entrainement de l'agent\n",
    "\n",
    "Il faut d'abbord commencer par entrainer l'agent/IA. C'est à dire à chaque étape l'agent va prendre un choix et obtenir une récompense puis mettre à jours son réseaux de neuronnes afin de maximiser la récompense.\n",
    "\n",
    "Le code vient de & la docs ce trouve sur :\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html\n",
    "\n",
    "Tu auras des explications sur les paramètres et leur usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.991    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 23150    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 98       |\n",
      "----------------------------------\n",
      "Num timesteps: 100\n",
      "Best mean reward: -inf - Last mean reward per episode: 24.50\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.984    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 12968    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 170      |\n",
      "----------------------------------\n",
      "Num timesteps: 200\n",
      "Best mean reward: 24.50 - Last mean reward per episode: 21.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.975    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 11580    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 263      |\n",
      "----------------------------------\n",
      "Num timesteps: 300\n",
      "Best mean reward: 24.50 - Last mean reward per episode: 22.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 12062    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 375      |\n",
      "----------------------------------\n",
      "Num timesteps: 400\n",
      "Best mean reward: 24.50 - Last mean reward per episode: 23.44\n",
      "Num timesteps: 500\n",
      "Best mean reward: 24.50 - Last mean reward per episode: 26.05\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 25.4     |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 11383    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 509      |\n",
      "----------------------------------\n",
      "Num timesteps: 600\n",
      "Best mean reward: 26.05 - Last mean reward per episode: 25.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 26.2     |\n",
      "|    exploration_rate | 0.94     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 11719    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 629      |\n",
      "----------------------------------\n",
      "Num timesteps: 700\n",
      "Best mean reward: 26.05 - Last mean reward per episode: 26.65\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.1     |\n",
      "|    ep_rew_mean      | 26.1     |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 11685    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 731      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.8     |\n",
      "|    ep_rew_mean      | 24.8     |\n",
      "|    exploration_rate | 0.925    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 11900    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 794      |\n",
      "----------------------------------\n",
      "Num timesteps: 800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.916    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 12011    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 881      |\n",
      "----------------------------------\n",
      "Num timesteps: 900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.908    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 11995    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 967      |\n",
      "----------------------------------\n",
      "Num timesteps: 1000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.899    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 12216    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1068     |\n",
      "----------------------------------\n",
      "Num timesteps: 1100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.89     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 12389    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1157     |\n",
      "----------------------------------\n",
      "Num timesteps: 1200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.881    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 12157    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1250     |\n",
      "----------------------------------\n",
      "Num timesteps: 1300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 12245    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1326     |\n",
      "----------------------------------\n",
      "Num timesteps: 1400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 12137    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1448     |\n",
      "----------------------------------\n",
      "Num timesteps: 1500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.855    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 12199    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1524     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 12390    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1599     |\n",
      "----------------------------------\n",
      "Num timesteps: 1600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.84     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 12378    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1685     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.832    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 12466    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1770     |\n",
      "----------------------------------\n",
      "Num timesteps: 1800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.825    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 12358    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1843     |\n",
      "----------------------------------\n",
      "Num timesteps: 1900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.817    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 12389    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1925     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.811    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 12516    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1985     |\n",
      "----------------------------------\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.803    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 12503    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2070     |\n",
      "----------------------------------\n",
      "Num timesteps: 2100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 12539    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2139     |\n",
      "----------------------------------\n",
      "Num timesteps: 2200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.79     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 12528    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2208     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.784    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 12375    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2272     |\n",
      "----------------------------------\n",
      "Num timesteps: 2300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.775    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 12203    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2364     |\n",
      "----------------------------------\n",
      "Num timesteps: 2400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 12260    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2461     |\n",
      "----------------------------------\n",
      "Num timesteps: 2500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.759    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 12305    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2535     |\n",
      "----------------------------------\n",
      "Num timesteps: 2600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.748    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 12362    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2648     |\n",
      "----------------------------------\n",
      "Num timesteps: 2700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.741    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 12395    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2722     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.734    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 12540    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2798     |\n",
      "----------------------------------\n",
      "Num timesteps: 2800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.67\n",
      "Num timesteps: 2900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.723    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 12500    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2918     |\n",
      "----------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.711    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 12633    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3042     |\n",
      "----------------------------------\n",
      "Num timesteps: 3100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.7      |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 12673    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3154     |\n",
      "----------------------------------\n",
      "Num timesteps: 3200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.691    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 12723    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3248     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 3300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.684    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 12738    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3329     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.678    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 12811    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3391     |\n",
      "----------------------------------\n",
      "Num timesteps: 3400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.671    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 12786    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3458     |\n",
      "----------------------------------\n",
      "Num timesteps: 3500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.66     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 12842    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3579     |\n",
      "----------------------------------\n",
      "Num timesteps: 3600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.651    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 12778    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3676     |\n",
      "----------------------------------\n",
      "Num timesteps: 3700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.643    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 12793    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3759     |\n",
      "----------------------------------\n",
      "Num timesteps: 3800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.636    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 12774    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3828     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.63     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 12858    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3890     |\n",
      "----------------------------------\n",
      "Num timesteps: 3900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.20\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.619    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 12697    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4009     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.614    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 12769    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4068     |\n",
      "----------------------------------\n",
      "Num timesteps: 4100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.605    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 12797    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4153     |\n",
      "----------------------------------\n",
      "Num timesteps: 4200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.597    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 12769    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4238     |\n",
      "----------------------------------\n",
      "Num timesteps: 4300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.591    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 12754    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4304     |\n",
      "----------------------------------\n",
      "Num timesteps: 4400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.582    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 12755    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4404     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.576    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 12814    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4467     |\n",
      "----------------------------------\n",
      "Num timesteps: 4500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.566    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 12830    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4573     |\n",
      "----------------------------------\n",
      "Num timesteps: 4600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.558    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 12787    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4656     |\n",
      "----------------------------------\n",
      "Num timesteps: 4700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.549    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 12788    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4743     |\n",
      "----------------------------------\n",
      "Num timesteps: 4800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.541    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 12784    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4827     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.535    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 12819    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4895     |\n",
      "----------------------------------\n",
      "Num timesteps: 4900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.529    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 12780    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4957     |\n",
      "----------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.518    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 12798    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5070     |\n",
      "----------------------------------\n",
      "Num timesteps: 5100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.511    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 12740    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5147     |\n",
      "----------------------------------\n",
      "Num timesteps: 5200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.502    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 12695    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5240     |\n",
      "----------------------------------\n",
      "Num timesteps: 5300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.494    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 12569    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5324     |\n",
      "----------------------------------\n",
      "Num timesteps: 5400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.483    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 12596    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5437     |\n",
      "----------------------------------\n",
      "Num timesteps: 5500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.475    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 12626    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5529     |\n",
      "----------------------------------\n",
      "Num timesteps: 5600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.468    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 12614    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5600     |\n",
      "----------------------------------\n",
      "Num timesteps: 5700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.457    |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 12611    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5713     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.452    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 12651    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5770     |\n",
      "----------------------------------\n",
      "Num timesteps: 5800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.445    |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 12620    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5844     |\n",
      "----------------------------------\n",
      "Num timesteps: 5900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.432    |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 12667    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 5982     |\n",
      "----------------------------------\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.421    |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 12680    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6099     |\n",
      "----------------------------------\n",
      "Num timesteps: 6100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.416    |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 12640    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6149     |\n",
      "----------------------------------\n",
      "Num timesteps: 6200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.411    |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 12614    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6204     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.403    |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 12661    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6284     |\n",
      "----------------------------------\n",
      "Num timesteps: 6300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.396    |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 12645    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6357     |\n",
      "----------------------------------\n",
      "Num timesteps: 6400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.388    |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 12653    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6445     |\n",
      "----------------------------------\n",
      "Num timesteps: 6500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.378    |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 12666    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6547     |\n",
      "----------------------------------\n",
      "Num timesteps: 6600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.372    |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 12668    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6610     |\n",
      "----------------------------------\n",
      "Num timesteps: 6700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.363    |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 12634    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6708     |\n",
      "----------------------------------\n",
      "Num timesteps: 6800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.347    |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 12712    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6871     |\n",
      "----------------------------------\n",
      "Num timesteps: 6900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.34     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 12656    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6943     |\n",
      "----------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.334    |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 12641    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7008     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.328    |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 12673    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7070     |\n",
      "----------------------------------\n",
      "Num timesteps: 7100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.316    |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 12686    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7195     |\n",
      "----------------------------------\n",
      "Num timesteps: 7200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.309    |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 12692    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7271     |\n",
      "----------------------------------\n",
      "Num timesteps: 7300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.302    |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 12658    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7345     |\n",
      "----------------------------------\n",
      "Num timesteps: 7400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.294    |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 12656    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7432     |\n",
      "----------------------------------\n",
      "Num timesteps: 7500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 12643    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7500     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.281    |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 12671    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7567     |\n",
      "----------------------------------\n",
      "Num timesteps: 7600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.27     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 12688    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7689     |\n",
      "----------------------------------\n",
      "Num timesteps: 7700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 12668    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7764     |\n",
      "----------------------------------\n",
      "Num timesteps: 7800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.253    |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 12677    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7864     |\n",
      "----------------------------------\n",
      "Num timesteps: 7900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.245    |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 12679    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7945     |\n",
      "----------------------------------\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.238    |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 12655    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8026     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 8100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.228    |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 12662    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8125     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.224    |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 12671    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8170     |\n",
      "----------------------------------\n",
      "Num timesteps: 8200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.217    |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 12645    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8243     |\n",
      "----------------------------------\n",
      "Num timesteps: 8300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.21     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 12631    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8316     |\n",
      "----------------------------------\n",
      "Num timesteps: 8400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.202    |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 12618    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8400     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.196    |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 12654    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8466     |\n",
      "----------------------------------\n",
      "Num timesteps: 8500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.189    |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 12655    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8533     |\n",
      "----------------------------------\n",
      "Num timesteps: 8600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.182    |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 12630    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8607     |\n",
      "----------------------------------\n",
      "Num timesteps: 8700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.173    |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 12650    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8702     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.166    |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 12695    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8774     |\n",
      "----------------------------------\n",
      "Num timesteps: 8800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.158    |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 12662    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8865     |\n",
      "----------------------------------\n",
      "Num timesteps: 8900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.148    |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 12679    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8965     |\n",
      "----------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.142    |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 12668    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9036     |\n",
      "----------------------------------\n",
      "Num timesteps: 9100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.129    |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 12715    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9164     |\n",
      "----------------------------------\n",
      "Num timesteps: 9200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.119    |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 12741    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9271     |\n",
      "----------------------------------\n",
      "Num timesteps: 9300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.67\n",
      "Num timesteps: 9400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.106    |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 12721    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9411     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 12750    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9474     |\n",
      "----------------------------------\n",
      "Num timesteps: 9500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.0941   |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 12650    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9536     |\n",
      "----------------------------------\n",
      "Num timesteps: 9600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.0859   |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 12564    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9622     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.0802   |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 12547    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9682     |\n",
      "----------------------------------\n",
      "Num timesteps: 9700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.072    |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 12461    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9768     |\n",
      "----------------------------------\n",
      "Num timesteps: 9800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.0627   |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 12476    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9866     |\n",
      "----------------------------------\n",
      "Num timesteps: 9900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.0555   |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 12439    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9942     |\n",
      "----------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 12416    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10010    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 12376    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10085    |\n",
      "----------------------------------\n",
      "Num timesteps: 10100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 12294    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10195    |\n",
      "----------------------------------\n",
      "Num timesteps: 10200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 12204    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10254    |\n",
      "----------------------------------\n",
      "Num timesteps: 10300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 12112    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10350    |\n",
      "----------------------------------\n",
      "Num timesteps: 10400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 12044    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10415    |\n",
      "----------------------------------\n",
      "Num timesteps: 10500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 11986    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10545    |\n",
      "----------------------------------\n",
      "Num timesteps: 10600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 11926    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10634    |\n",
      "----------------------------------\n",
      "Num timesteps: 10700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 11887    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10702    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 11931    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10793    |\n",
      "----------------------------------\n",
      "Num timesteps: 10800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 11927    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10855    |\n",
      "----------------------------------\n",
      "Num timesteps: 10900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 11898    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 10910    |\n",
      "----------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 11933    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11033    |\n",
      "----------------------------------\n",
      "Num timesteps: 11100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 11912    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11107    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 11957    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11199    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 11200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 11953    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11253    |\n",
      "----------------------------------\n",
      "Num timesteps: 11300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 11926    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11364    |\n",
      "----------------------------------\n",
      "Num timesteps: 11400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 11946    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11492    |\n",
      "----------------------------------\n",
      "Num timesteps: 11500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.81\n",
      "Num timesteps: 11600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 11942    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11616    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 11974    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11688    |\n",
      "----------------------------------\n",
      "Num timesteps: 11700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 11922    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11789    |\n",
      "----------------------------------\n",
      "Num timesteps: 11800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 11903    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 11885    |\n",
      "----------------------------------\n",
      "Num timesteps: 11900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 11904    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 11949    |\n",
      "----------------------------------\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 11920    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12038    |\n",
      "----------------------------------\n",
      "Num timesteps: 12100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 11920    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12129    |\n",
      "----------------------------------\n",
      "Num timesteps: 12200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 11941    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12249    |\n",
      "----------------------------------\n",
      "Num timesteps: 12300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 11930    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12341    |\n",
      "----------------------------------\n",
      "Num timesteps: 12400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 11929    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12416    |\n",
      "----------------------------------\n",
      "Num timesteps: 12500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 11939    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12530    |\n",
      "----------------------------------\n",
      "Num timesteps: 12600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 11939    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12600    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 11967    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12681    |\n",
      "----------------------------------\n",
      "Num timesteps: 12700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 11967    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12791    |\n",
      "----------------------------------\n",
      "Num timesteps: 12800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 11925    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12880    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 12900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 11930    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 12972    |\n",
      "----------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 11923    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13036    |\n",
      "----------------------------------\n",
      "Num timesteps: 13100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13100    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 11934    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13176    |\n",
      "----------------------------------\n",
      "Num timesteps: 13200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 11917    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13248    |\n",
      "----------------------------------\n",
      "Num timesteps: 13300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 11896    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13314    |\n",
      "----------------------------------\n",
      "Num timesteps: 13400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 11898    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13411    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13487    |\n",
      "----------------------------------\n",
      "Num timesteps: 13500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.34\n",
      "Num timesteps: 13600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 11879    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13608    |\n",
      "----------------------------------\n",
      "Num timesteps: 13700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13772    |\n",
      "----------------------------------\n",
      "Num timesteps: 13800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 11891    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13839    |\n",
      "----------------------------------\n",
      "Num timesteps: 13900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 11895    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13930    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 11908    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 13981    |\n",
      "----------------------------------\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 11882    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14059    |\n",
      "----------------------------------\n",
      "Num timesteps: 14100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 11883    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14149    |\n",
      "----------------------------------\n",
      "Num timesteps: 14200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 11848    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14232    |\n",
      "----------------------------------\n",
      "Num timesteps: 14300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 11873    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14390    |\n",
      "----------------------------------\n",
      "Num timesteps: 14400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14491    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 14500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 11849    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14557    |\n",
      "----------------------------------\n",
      "Num timesteps: 14600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 11833    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14635    |\n",
      "----------------------------------\n",
      "Num timesteps: 14700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14742    |\n",
      "----------------------------------\n",
      "Num timesteps: 14800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 11839    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14879    |\n",
      "----------------------------------\n",
      "Num timesteps: 14900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 11835    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 14961    |\n",
      "----------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 11834    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15039    |\n",
      "----------------------------------\n",
      "Num timesteps: 15100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 11833    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15115    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 11843    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15180    |\n",
      "----------------------------------\n",
      "Num timesteps: 15200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 11832    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15241    |\n",
      "----------------------------------\n",
      "Num timesteps: 15300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 11807    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15340    |\n",
      "----------------------------------\n",
      "Num timesteps: 15400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 11813    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15434    |\n",
      "----------------------------------\n",
      "Num timesteps: 15500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 11803    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15505    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15575    |\n",
      "----------------------------------\n",
      "Num timesteps: 15600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 11828    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15656    |\n",
      "----------------------------------\n",
      "Num timesteps: 15700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 11820    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15726    |\n",
      "----------------------------------\n",
      "Num timesteps: 15800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 11820    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15804    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 11845    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15878    |\n",
      "----------------------------------\n",
      "Num timesteps: 15900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 15997    |\n",
      "----------------------------------\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 16100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 11865    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16140    |\n",
      "----------------------------------\n",
      "Num timesteps: 16200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 11862    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16238    |\n",
      "----------------------------------\n",
      "Num timesteps: 16300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 11874    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16323    |\n",
      "----------------------------------\n",
      "Num timesteps: 16400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.07\n",
      "Num timesteps: 16500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 11878    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16502    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 11867    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16578    |\n",
      "----------------------------------\n",
      "Num timesteps: 16600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 11872    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16688    |\n",
      "----------------------------------\n",
      "Num timesteps: 16700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 11875    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16770    |\n",
      "----------------------------------\n",
      "Num timesteps: 16800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 11880    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16865    |\n",
      "----------------------------------\n",
      "Num timesteps: 16900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 11890    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 16964    |\n",
      "----------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 11893    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17082    |\n",
      "----------------------------------\n",
      "Num timesteps: 17100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 11892    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17153    |\n",
      "----------------------------------\n",
      "Num timesteps: 17200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 11884    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17241    |\n",
      "----------------------------------\n",
      "Num timesteps: 17300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 11831    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17340    |\n",
      "----------------------------------\n",
      "Num timesteps: 17400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17433    |\n",
      "----------------------------------\n",
      "Num timesteps: 17500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 11770    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17513    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 11771    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17591    |\n",
      "----------------------------------\n",
      "Num timesteps: 17600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 804      |\n",
      "|    fps              | 11731    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17675    |\n",
      "----------------------------------\n",
      "Num timesteps: 17700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 808      |\n",
      "|    fps              | 11720    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17792    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 17800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 812      |\n",
      "|    fps              | 11723    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17873    |\n",
      "----------------------------------\n",
      "Num timesteps: 17900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 816      |\n",
      "|    fps              | 11712    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 17950    |\n",
      "----------------------------------\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 820      |\n",
      "|    fps              | 11716    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18034    |\n",
      "----------------------------------\n",
      "Num timesteps: 18100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 824      |\n",
      "|    fps              | 11717    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18134    |\n",
      "----------------------------------\n",
      "Num timesteps: 18200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 828      |\n",
      "|    fps              | 11725    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18227    |\n",
      "----------------------------------\n",
      "Num timesteps: 18300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 24.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 832      |\n",
      "|    fps              | 11737    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18336    |\n",
      "----------------------------------\n",
      "Num timesteps: 18400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 836      |\n",
      "|    fps              | 11769    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18495    |\n",
      "----------------------------------\n",
      "Num timesteps: 18500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 840      |\n",
      "|    fps              | 11768    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18560    |\n",
      "----------------------------------\n",
      "Num timesteps: 18600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 844      |\n",
      "|    fps              | 11773    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18670    |\n",
      "----------------------------------\n",
      "Num timesteps: 18700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 848      |\n",
      "|    fps              | 11773    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18738    |\n",
      "----------------------------------\n",
      "Num timesteps: 18800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 852      |\n",
      "|    fps              | 11764    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18829    |\n",
      "----------------------------------\n",
      "Num timesteps: 18900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 856      |\n",
      "|    fps              | 11770    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18914    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 860      |\n",
      "|    fps              | 11787    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 18985    |\n",
      "----------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 864      |\n",
      "|    fps              | 11786    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19074    |\n",
      "----------------------------------\n",
      "Num timesteps: 19100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 868      |\n",
      "|    fps              | 11793    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19154    |\n",
      "----------------------------------\n",
      "Num timesteps: 19200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 872      |\n",
      "|    fps              | 11785    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19241    |\n",
      "----------------------------------\n",
      "Num timesteps: 19300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 876      |\n",
      "|    fps              | 11785    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19300    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 880      |\n",
      "|    fps              | 11804    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19369    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 19400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 884      |\n",
      "|    fps              | 11811    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19470    |\n",
      "----------------------------------\n",
      "Num timesteps: 19500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 888      |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19576    |\n",
      "----------------------------------\n",
      "Num timesteps: 19600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 892      |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19658    |\n",
      "----------------------------------\n",
      "Num timesteps: 19700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 896      |\n",
      "|    fps              | 11817    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19716    |\n",
      "----------------------------------\n",
      "Num timesteps: 19800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 900      |\n",
      "|    fps              | 11823    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19813    |\n",
      "----------------------------------\n",
      "Num timesteps: 19900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 904      |\n",
      "|    fps              | 11802    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 19908    |\n",
      "----------------------------------\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 908      |\n",
      "|    fps              | 11799    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20037    |\n",
      "----------------------------------\n",
      "Num timesteps: 20100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 912      |\n",
      "|    fps              | 11769    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20105    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 916      |\n",
      "|    fps              | 11759    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20153    |\n",
      "----------------------------------\n",
      "Num timesteps: 20200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 920      |\n",
      "|    fps              | 11756    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20224    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 924      |\n",
      "|    fps              | 11776    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20297    |\n",
      "----------------------------------\n",
      "Num timesteps: 20300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 928      |\n",
      "|    fps              | 11780    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20374    |\n",
      "----------------------------------\n",
      "Num timesteps: 20400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 932      |\n",
      "|    fps              | 11786    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20492    |\n",
      "----------------------------------\n",
      "Num timesteps: 20500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 936      |\n",
      "|    fps              | 11791    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20577    |\n",
      "----------------------------------\n",
      "Num timesteps: 20600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 940      |\n",
      "|    fps              | 11781    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20640    |\n",
      "----------------------------------\n",
      "Num timesteps: 20700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 944      |\n",
      "|    fps              | 11795    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20754    |\n",
      "----------------------------------\n",
      "Num timesteps: 20800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 948      |\n",
      "|    fps              | 11800    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20869    |\n",
      "----------------------------------\n",
      "Num timesteps: 20900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 952      |\n",
      "|    fps              | 11797    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 20941    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 21000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 956      |\n",
      "|    fps              | 11794    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21042    |\n",
      "----------------------------------\n",
      "Num timesteps: 21100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 960      |\n",
      "|    fps              | 11795    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21104    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 964      |\n",
      "|    fps              | 11818    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21191    |\n",
      "----------------------------------\n",
      "Num timesteps: 21200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 968      |\n",
      "|    fps              | 11818    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21299    |\n",
      "----------------------------------\n",
      "Num timesteps: 21300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 972      |\n",
      "|    fps              | 11818    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21378    |\n",
      "----------------------------------\n",
      "Num timesteps: 21400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 976      |\n",
      "|    fps              | 11811    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21474    |\n",
      "----------------------------------\n",
      "Num timesteps: 21500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 980      |\n",
      "|    fps              | 11808    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21540    |\n",
      "----------------------------------\n",
      "Num timesteps: 21600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 984      |\n",
      "|    fps              | 11795    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21603    |\n",
      "----------------------------------\n",
      "Num timesteps: 21700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 988      |\n",
      "|    fps              | 11766    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21748    |\n",
      "----------------------------------\n",
      "Num timesteps: 21800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 992      |\n",
      "|    fps              | 11774    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21839    |\n",
      "----------------------------------\n",
      "Num timesteps: 21900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 996      |\n",
      "|    fps              | 11781    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 21937    |\n",
      "----------------------------------\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1000     |\n",
      "|    fps              | 11777    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22013    |\n",
      "----------------------------------\n",
      "Num timesteps: 22100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1004     |\n",
      "|    fps              | 11786    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22104    |\n",
      "----------------------------------\n",
      "Num timesteps: 22200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1008     |\n",
      "|    fps              | 11797    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22233    |\n",
      "----------------------------------\n",
      "Num timesteps: 22300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1012     |\n",
      "|    fps              | 11813    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22353    |\n",
      "----------------------------------\n",
      "Num timesteps: 22400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1016     |\n",
      "|    fps              | 11795    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22401    |\n",
      "----------------------------------\n",
      "Num timesteps: 22500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1020     |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22507    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1024     |\n",
      "|    fps              | 11823    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22570    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 22600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1028     |\n",
      "|    fps              | 11820    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22669    |\n",
      "----------------------------------\n",
      "Num timesteps: 22700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1032     |\n",
      "|    fps              | 11825    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22747    |\n",
      "----------------------------------\n",
      "Num timesteps: 22800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1036     |\n",
      "|    fps              | 11832    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22879    |\n",
      "----------------------------------\n",
      "Num timesteps: 22900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1040     |\n",
      "|    fps              | 11842    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 22988    |\n",
      "----------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1044     |\n",
      "|    fps              | 11837    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23057    |\n",
      "----------------------------------\n",
      "Num timesteps: 23100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1048     |\n",
      "|    fps              | 11841    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23141    |\n",
      "----------------------------------\n",
      "Num timesteps: 23200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1052     |\n",
      "|    fps              | 11848    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23233    |\n",
      "----------------------------------\n",
      "Num timesteps: 23300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1056     |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23343    |\n",
      "----------------------------------\n",
      "Num timesteps: 23400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1060     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23446    |\n",
      "----------------------------------\n",
      "Num timesteps: 23500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1064     |\n",
      "|    fps              | 11852    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23508    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1068     |\n",
      "|    fps              | 11867    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23581    |\n",
      "----------------------------------\n",
      "Num timesteps: 23600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1072     |\n",
      "|    fps              | 11867    |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total_timesteps  | 23645    |\n",
      "----------------------------------\n",
      "Num timesteps: 23700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1076     |\n",
      "|    fps              | 11873    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 23766    |\n",
      "----------------------------------\n",
      "Num timesteps: 23800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.79\n",
      "Num timesteps: 23900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1080     |\n",
      "|    fps              | 11867    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 23915    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1084     |\n",
      "|    fps              | 11884    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 23994    |\n",
      "----------------------------------\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1088     |\n",
      "|    fps              | 11889    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24088    |\n",
      "----------------------------------\n",
      "Num timesteps: 24100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1092     |\n",
      "|    fps              | 11885    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24169    |\n",
      "----------------------------------\n",
      "Num timesteps: 24200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1096     |\n",
      "|    fps              | 11889    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24243    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 24300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1100     |\n",
      "|    fps              | 11904    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24386    |\n",
      "----------------------------------\n",
      "Num timesteps: 24400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1104     |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24493    |\n",
      "----------------------------------\n",
      "Num timesteps: 24500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1108     |\n",
      "|    fps              | 11910    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24576    |\n",
      "----------------------------------\n",
      "Num timesteps: 24600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1112     |\n",
      "|    fps              | 11898    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24624    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1116     |\n",
      "|    fps              | 11912    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24697    |\n",
      "----------------------------------\n",
      "Num timesteps: 24700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1120     |\n",
      "|    fps              | 11892    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24792    |\n",
      "----------------------------------\n",
      "Num timesteps: 24800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1124     |\n",
      "|    fps              | 11894    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24882    |\n",
      "----------------------------------\n",
      "Num timesteps: 24900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1128     |\n",
      "|    fps              | 11879    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 24946    |\n",
      "----------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1132     |\n",
      "|    fps              | 11892    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25063    |\n",
      "----------------------------------\n",
      "Num timesteps: 25100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1136     |\n",
      "|    fps              | 11881    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25144    |\n",
      "----------------------------------\n",
      "Num timesteps: 25200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1140     |\n",
      "|    fps              | 11887    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25227    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1144     |\n",
      "|    fps              | 11898    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25283    |\n",
      "----------------------------------\n",
      "Num timesteps: 25300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1148     |\n",
      "|    fps              | 11894    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25375    |\n",
      "----------------------------------\n",
      "Num timesteps: 25400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1152     |\n",
      "|    fps              | 11897    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25458    |\n",
      "----------------------------------\n",
      "Num timesteps: 25500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1156     |\n",
      "|    fps              | 11901    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25553    |\n",
      "----------------------------------\n",
      "Num timesteps: 25600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1160     |\n",
      "|    fps              | 11887    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25621    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1164     |\n",
      "|    fps              | 11880    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25684    |\n",
      "----------------------------------\n",
      "Num timesteps: 25700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1168     |\n",
      "|    fps              | 11866    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25780    |\n",
      "----------------------------------\n",
      "Num timesteps: 25800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1172     |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25871    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 25900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1176     |\n",
      "|    fps              | 11831    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 25951    |\n",
      "----------------------------------\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.9     |\n",
      "|    ep_rew_mean      | 20.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1180     |\n",
      "|    fps              | 11807    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26002    |\n",
      "----------------------------------\n",
      "Num timesteps: 26100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1184     |\n",
      "|    fps              | 11790    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26136    |\n",
      "----------------------------------\n",
      "Num timesteps: 26200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.28\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1188     |\n",
      "|    fps              | 11796    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26218    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1192     |\n",
      "|    fps              | 11812    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26293    |\n",
      "----------------------------------\n",
      "Num timesteps: 26300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1196     |\n",
      "|    fps              | 11783    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26352    |\n",
      "----------------------------------\n",
      "Num timesteps: 26400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1200     |\n",
      "|    fps              | 11754    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26422    |\n",
      "----------------------------------\n",
      "Num timesteps: 26500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1204     |\n",
      "|    fps              | 11753    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26522    |\n",
      "----------------------------------\n",
      "Num timesteps: 26600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1208     |\n",
      "|    fps              | 11755    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26601    |\n",
      "----------------------------------\n",
      "Num timesteps: 26700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1212     |\n",
      "|    fps              | 11760    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26707    |\n",
      "----------------------------------\n",
      "Num timesteps: 26800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1216     |\n",
      "|    fps              | 11762    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26802    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1220     |\n",
      "|    fps              | 11761    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26872    |\n",
      "----------------------------------\n",
      "Num timesteps: 26900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1224     |\n",
      "|    fps              | 11767    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 26965    |\n",
      "----------------------------------\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1228     |\n",
      "|    fps              | 11744    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27030    |\n",
      "----------------------------------\n",
      "Num timesteps: 27100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1232     |\n",
      "|    fps              | 11745    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27120    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1236     |\n",
      "|    fps              | 11749    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27178    |\n",
      "----------------------------------\n",
      "Num timesteps: 27200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1240     |\n",
      "|    fps              | 11751    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27265    |\n",
      "----------------------------------\n",
      "Num timesteps: 27300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1244     |\n",
      "|    fps              | 11756    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27347    |\n",
      "----------------------------------\n",
      "Num timesteps: 27400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1248     |\n",
      "|    fps              | 11751    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27455    |\n",
      "----------------------------------\n",
      "Num timesteps: 27500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1252     |\n",
      "|    fps              | 11752    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27534    |\n",
      "----------------------------------\n",
      "Num timesteps: 27600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1256     |\n",
      "|    fps              | 11754    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27630    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1260     |\n",
      "|    fps              | 11764    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27696    |\n",
      "----------------------------------\n",
      "Num timesteps: 27700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1264     |\n",
      "|    fps              | 11767    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27799    |\n",
      "----------------------------------\n",
      "Num timesteps: 27800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.15\n",
      "Num timesteps: 27900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1268     |\n",
      "|    fps              | 11755    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27900    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1272     |\n",
      "|    fps              | 11772    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 27992    |\n",
      "----------------------------------\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.21\n",
      "Num timesteps: 28100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1276     |\n",
      "|    fps              | 11768    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28100    |\n",
      "----------------------------------\n",
      "Num timesteps: 28200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1280     |\n",
      "|    fps              | 11777    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28215    |\n",
      "----------------------------------\n",
      "Num timesteps: 28300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1284     |\n",
      "|    fps              | 11789    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28353    |\n",
      "----------------------------------\n",
      "Num timesteps: 28400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1288     |\n",
      "|    fps              | 11795    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28447    |\n",
      "----------------------------------\n",
      "Num timesteps: 28500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1292     |\n",
      "|    fps              | 11792    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1296     |\n",
      "|    fps              | 11788    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28590    |\n",
      "----------------------------------\n",
      "Num timesteps: 28600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1300     |\n",
      "|    fps              | 11774    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28665    |\n",
      "----------------------------------\n",
      "Num timesteps: 28700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1304     |\n",
      "|    fps              | 11768    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28718    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1308     |\n",
      "|    fps              | 11776    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28767    |\n",
      "----------------------------------\n",
      "Num timesteps: 28800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1312     |\n",
      "|    fps              | 11767    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28854    |\n",
      "----------------------------------\n",
      "Num timesteps: 28900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1316     |\n",
      "|    fps              | 11765    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 28914    |\n",
      "----------------------------------\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1320     |\n",
      "|    fps              | 11765    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29025    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 29100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1324     |\n",
      "|    fps              | 11765    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29106    |\n",
      "----------------------------------\n",
      "Num timesteps: 29200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1328     |\n",
      "|    fps              | 11767    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29243    |\n",
      "----------------------------------\n",
      "Num timesteps: 29300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1332     |\n",
      "|    fps              | 11768    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29313    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1336     |\n",
      "|    fps              | 11780    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29385    |\n",
      "----------------------------------\n",
      "Num timesteps: 29400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1340     |\n",
      "|    fps              | 11775    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29456    |\n",
      "----------------------------------\n",
      "Num timesteps: 29500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1344     |\n",
      "|    fps              | 11774    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29526    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1348     |\n",
      "|    fps              | 11785    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29596    |\n",
      "----------------------------------\n",
      "Num timesteps: 29600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.41\n",
      "Num timesteps: 29700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1352     |\n",
      "|    fps              | 11771    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29711    |\n",
      "----------------------------------\n",
      "Num timesteps: 29800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1356     |\n",
      "|    fps              | 11769    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29815    |\n",
      "----------------------------------\n",
      "Num timesteps: 29900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1360     |\n",
      "|    fps              | 11773    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 29907    |\n",
      "----------------------------------\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1364     |\n",
      "|    fps              | 11770    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30026    |\n",
      "----------------------------------\n",
      "Num timesteps: 30100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1368     |\n",
      "|    fps              | 11781    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30150    |\n",
      "----------------------------------\n",
      "Num timesteps: 30200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1372     |\n",
      "|    fps              | 11771    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30223    |\n",
      "----------------------------------\n",
      "Num timesteps: 30300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1376     |\n",
      "|    fps              | 11773    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30312    |\n",
      "----------------------------------\n",
      "Num timesteps: 30400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1380     |\n",
      "|    fps              | 11772    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30416    |\n",
      "----------------------------------\n",
      "Num timesteps: 30500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1384     |\n",
      "|    fps              | 11777    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30524    |\n",
      "----------------------------------\n",
      "Num timesteps: 30600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1388     |\n",
      "|    fps              | 11771    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30601    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1392     |\n",
      "|    fps              | 11779    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30657    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 30700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1396     |\n",
      "|    fps              | 11780    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30741    |\n",
      "----------------------------------\n",
      "Num timesteps: 30800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1400     |\n",
      "|    fps              | 11763    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30801    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1404     |\n",
      "|    fps              | 11776    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30882    |\n",
      "----------------------------------\n",
      "Num timesteps: 30900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1408     |\n",
      "|    fps              | 11777    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 30962    |\n",
      "----------------------------------\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1412     |\n",
      "|    fps              | 11781    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31078    |\n",
      "----------------------------------\n",
      "Num timesteps: 31100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1416     |\n",
      "|    fps              | 11787    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31197    |\n",
      "----------------------------------\n",
      "Num timesteps: 31200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1420     |\n",
      "|    fps              | 11786    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31274    |\n",
      "----------------------------------\n",
      "Num timesteps: 31300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1424     |\n",
      "|    fps              | 11797    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31388    |\n",
      "----------------------------------\n",
      "Num timesteps: 31400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.82\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1428     |\n",
      "|    fps              | 11797    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31467    |\n",
      "----------------------------------\n",
      "Num timesteps: 31500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.34\n",
      "Num timesteps: 31600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1432     |\n",
      "|    fps              | 11804    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31607    |\n",
      "----------------------------------\n",
      "Num timesteps: 31700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1436     |\n",
      "|    fps              | 11797    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31708    |\n",
      "----------------------------------\n",
      "Num timesteps: 31800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1440     |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31848    |\n",
      "----------------------------------\n",
      "Num timesteps: 31900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 24.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1444     |\n",
      "|    fps              | 11823    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 31988    |\n",
      "----------------------------------\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1448     |\n",
      "|    fps              | 11827    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32062    |\n",
      "----------------------------------\n",
      "Num timesteps: 32100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.66\n",
      "Num timesteps: 32200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 25.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1452     |\n",
      "|    fps              | 11829    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32206    |\n",
      "----------------------------------\n",
      "Num timesteps: 32300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1456     |\n",
      "|    fps              | 11832    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32300    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.7     |\n",
      "|    ep_rew_mean      | 24.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1460     |\n",
      "|    fps              | 11842    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32377    |\n",
      "----------------------------------\n",
      "Num timesteps: 32400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1464     |\n",
      "|    fps              | 11843    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32460    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 32500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1468     |\n",
      "|    fps              | 11846    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32552    |\n",
      "----------------------------------\n",
      "Num timesteps: 32600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1472     |\n",
      "|    fps              | 11846    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32630    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1476     |\n",
      "|    fps              | 11855    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32689    |\n",
      "----------------------------------\n",
      "Num timesteps: 32700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.77\n",
      "Num timesteps: 32800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1480     |\n",
      "|    fps              | 11852    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32811    |\n",
      "----------------------------------\n",
      "Num timesteps: 32900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1484     |\n",
      "|    fps              | 11859    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32900    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1488     |\n",
      "|    fps              | 11870    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 32975    |\n",
      "----------------------------------\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1492     |\n",
      "|    fps              | 11876    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33081    |\n",
      "----------------------------------\n",
      "Num timesteps: 33100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1496     |\n",
      "|    fps              | 11873    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33127    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1500     |\n",
      "|    fps              | 11884    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33194    |\n",
      "----------------------------------\n",
      "Num timesteps: 33200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1504     |\n",
      "|    fps              | 11888    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33287    |\n",
      "----------------------------------\n",
      "Num timesteps: 33300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1508     |\n",
      "|    fps              | 11890    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33373    |\n",
      "----------------------------------\n",
      "Num timesteps: 33400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.89\n",
      "Num timesteps: 33500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1512     |\n",
      "|    fps              | 11889    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33507    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1516     |\n",
      "|    fps              | 11894    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33564    |\n",
      "----------------------------------\n",
      "Num timesteps: 33600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.67\n",
      "Num timesteps: 33700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.23\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1520     |\n",
      "|    fps              | 11891    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33705    |\n",
      "----------------------------------\n",
      "Num timesteps: 33800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.4     |\n",
      "|    ep_rew_mean      | 24.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1524     |\n",
      "|    fps              | 11895    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33824    |\n",
      "----------------------------------\n",
      "Num timesteps: 33900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1528     |\n",
      "|    fps              | 11893    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33915    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1532     |\n",
      "|    fps              | 11905    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 33995    |\n",
      "----------------------------------\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1536     |\n",
      "|    fps              | 11904    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34052    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 34100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1540     |\n",
      "|    fps              | 11904    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34137    |\n",
      "----------------------------------\n",
      "Num timesteps: 34200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1544     |\n",
      "|    fps              | 11904    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34204    |\n",
      "----------------------------------\n",
      "Num timesteps: 34300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1548     |\n",
      "|    fps              | 11906    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34308    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1552     |\n",
      "|    fps              | 11915    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34384    |\n",
      "----------------------------------\n",
      "Num timesteps: 34400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1556     |\n",
      "|    fps              | 11913    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34452    |\n",
      "----------------------------------\n",
      "Num timesteps: 34500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1560     |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34541    |\n",
      "----------------------------------\n",
      "Num timesteps: 34600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.57\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1564     |\n",
      "|    fps              | 11914    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34631    |\n",
      "----------------------------------\n",
      "Num timesteps: 34700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1568     |\n",
      "|    fps              | 11917    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34722    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1572     |\n",
      "|    fps              | 11925    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34794    |\n",
      "----------------------------------\n",
      "Num timesteps: 34800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1576     |\n",
      "|    fps              | 11923    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34866    |\n",
      "----------------------------------\n",
      "Num timesteps: 34900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1580     |\n",
      "|    fps              | 11922    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 34941    |\n",
      "----------------------------------\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1584     |\n",
      "|    fps              | 11922    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35016    |\n",
      "----------------------------------\n",
      "Num timesteps: 35100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1588     |\n",
      "|    fps              | 11926    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35106    |\n",
      "----------------------------------\n",
      "Num timesteps: 35200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1592     |\n",
      "|    fps              | 11935    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35239    |\n",
      "----------------------------------\n",
      "Num timesteps: 35300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1596     |\n",
      "|    fps              | 11934    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35303    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1600     |\n",
      "|    fps              | 11944    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35386    |\n",
      "----------------------------------\n",
      "Num timesteps: 35400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1604     |\n",
      "|    fps              | 11945    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35474    |\n",
      "----------------------------------\n",
      "Num timesteps: 35500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.87\n",
      "Num timesteps: 35600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1608     |\n",
      "|    fps              | 11951    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35648    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 35700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1612     |\n",
      "|    fps              | 11949    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35713    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1616     |\n",
      "|    fps              | 11961    |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total_timesteps  | 35796    |\n",
      "----------------------------------\n",
      "Num timesteps: 35800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.32\n",
      "Num timesteps: 35900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1620     |\n",
      "|    fps              | 11949    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 35906    |\n",
      "----------------------------------\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1624     |\n",
      "|    fps              | 11952    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36000    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1628     |\n",
      "|    fps              | 11953    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36060    |\n",
      "----------------------------------\n",
      "Num timesteps: 36100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1632     |\n",
      "|    fps              | 11953    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36134    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1636     |\n",
      "|    fps              | 11961    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36197    |\n",
      "----------------------------------\n",
      "Num timesteps: 36200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1640     |\n",
      "|    fps              | 11956    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36262    |\n",
      "----------------------------------\n",
      "Num timesteps: 36300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1644     |\n",
      "|    fps              | 11964    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36390    |\n",
      "----------------------------------\n",
      "Num timesteps: 36400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1648     |\n",
      "|    fps              | 11958    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36458    |\n",
      "----------------------------------\n",
      "Num timesteps: 36500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1652     |\n",
      "|    fps              | 11958    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36539    |\n",
      "----------------------------------\n",
      "Num timesteps: 36600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1656     |\n",
      "|    fps              | 11968    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36668    |\n",
      "----------------------------------\n",
      "Num timesteps: 36700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1660     |\n",
      "|    fps              | 11963    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36723    |\n",
      "----------------------------------\n",
      "Num timesteps: 36800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1664     |\n",
      "|    fps              | 11967    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36811    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1668     |\n",
      "|    fps              | 11977    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36895    |\n",
      "----------------------------------\n",
      "Num timesteps: 36900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1672     |\n",
      "|    fps              | 11969    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 36945    |\n",
      "----------------------------------\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.50\n",
      "Num timesteps: 37100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1676     |\n",
      "|    fps              | 11933    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37101    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1680     |\n",
      "|    fps              | 11928    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37186    |\n",
      "----------------------------------\n",
      "Num timesteps: 37200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1684     |\n",
      "|    fps              | 11911    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37255    |\n",
      "----------------------------------\n",
      "Num timesteps: 37300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1688     |\n",
      "|    fps              | 11891    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37332    |\n",
      "----------------------------------\n",
      "Num timesteps: 37400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1692     |\n",
      "|    fps              | 11875    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37417    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1696     |\n",
      "|    fps              | 11883    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37499    |\n",
      "----------------------------------\n",
      "Num timesteps: 37500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.96\n",
      "Num timesteps: 37600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1700     |\n",
      "|    fps              | 11874    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37615    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1704     |\n",
      "|    fps              | 11885    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37696    |\n",
      "----------------------------------\n",
      "Num timesteps: 37700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.22\n",
      "Num timesteps: 37800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.49\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1708     |\n",
      "|    fps              | 11874    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37802    |\n",
      "----------------------------------\n",
      "Num timesteps: 37900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1712     |\n",
      "|    fps              | 11881    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 37913    |\n",
      "----------------------------------\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1716     |\n",
      "|    fps              | 11880    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38001    |\n",
      "----------------------------------\n",
      "Num timesteps: 38100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1720     |\n",
      "|    fps              | 11876    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38101    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1724     |\n",
      "|    fps              | 11886    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38197    |\n",
      "----------------------------------\n",
      "Num timesteps: 38200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1728     |\n",
      "|    fps              | 11884    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38280    |\n",
      "----------------------------------\n",
      "Num timesteps: 38300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1732     |\n",
      "|    fps              | 11886    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38372    |\n",
      "----------------------------------\n",
      "Num timesteps: 38400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1736     |\n",
      "|    fps              | 11882    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38455    |\n",
      "----------------------------------\n",
      "Num timesteps: 38500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1740     |\n",
      "|    fps              | 11887    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38565    |\n",
      "----------------------------------\n",
      "Num timesteps: 38600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1744     |\n",
      "|    fps              | 11881    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38640    |\n",
      "----------------------------------\n",
      "Num timesteps: 38700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1748     |\n",
      "|    fps              | 11885    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38739    |\n",
      "----------------------------------\n",
      "Num timesteps: 38800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1752     |\n",
      "|    fps              | 11883    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38844    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 38900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1756     |\n",
      "|    fps              | 11881    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38910    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1760     |\n",
      "|    fps              | 11891    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 38992    |\n",
      "----------------------------------\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1764     |\n",
      "|    fps              | 11887    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39080    |\n",
      "----------------------------------\n",
      "Num timesteps: 39100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1768     |\n",
      "|    fps              | 11885    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39148    |\n",
      "----------------------------------\n",
      "Num timesteps: 39200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1772     |\n",
      "|    fps              | 11890    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39297    |\n",
      "----------------------------------\n",
      "Num timesteps: 39300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1776     |\n",
      "|    fps              | 11884    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39350    |\n",
      "----------------------------------\n",
      "Num timesteps: 39400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1780     |\n",
      "|    fps              | 11874    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39417    |\n",
      "----------------------------------\n",
      "Num timesteps: 39500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1784     |\n",
      "|    fps              | 11883    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39571    |\n",
      "----------------------------------\n",
      "Num timesteps: 39600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1788     |\n",
      "|    fps              | 11885    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39687    |\n",
      "----------------------------------\n",
      "Num timesteps: 39700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1792     |\n",
      "|    fps              | 11883    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39755    |\n",
      "----------------------------------\n",
      "Num timesteps: 39800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1796     |\n",
      "|    fps              | 11880    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39858    |\n",
      "----------------------------------\n",
      "Num timesteps: 39900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1800     |\n",
      "|    fps              | 11868    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 39981    |\n",
      "----------------------------------\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1804     |\n",
      "|    fps              | 11865    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40067    |\n",
      "----------------------------------\n",
      "Num timesteps: 40100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.71\n",
      "Num timesteps: 40200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1808     |\n",
      "|    fps              | 11855    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40212    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1812     |\n",
      "|    fps              | 11862    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40290    |\n",
      "----------------------------------\n",
      "Num timesteps: 40300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1816     |\n",
      "|    fps              | 11856    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40358    |\n",
      "----------------------------------\n",
      "Num timesteps: 40400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1820     |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40433    |\n",
      "----------------------------------\n",
      "Num timesteps: 40500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1824     |\n",
      "|    fps              | 11852    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40515    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1828     |\n",
      "|    fps              | 11860    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40576    |\n",
      "----------------------------------\n",
      "Num timesteps: 40600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1832     |\n",
      "|    fps              | 11862    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40653    |\n",
      "----------------------------------\n",
      "Num timesteps: 40700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1836     |\n",
      "|    fps              | 11864    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40764    |\n",
      "----------------------------------\n",
      "Num timesteps: 40800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1840     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40830    |\n",
      "----------------------------------\n",
      "Num timesteps: 40900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1844     |\n",
      "|    fps              | 11858    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 40928    |\n",
      "----------------------------------\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1848     |\n",
      "|    fps              | 11861    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41019    |\n",
      "----------------------------------\n",
      "Num timesteps: 41100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1852     |\n",
      "|    fps              | 11859    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41105    |\n",
      "----------------------------------\n",
      "Num timesteps: 41200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1856     |\n",
      "|    fps              | 11864    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41205    |\n",
      "----------------------------------\n",
      "Num timesteps: 41300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1860     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41304    |\n",
      "----------------------------------\n",
      "Num timesteps: 41400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1864     |\n",
      "|    fps              | 11866    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41408    |\n",
      "----------------------------------\n",
      "Num timesteps: 41500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1868     |\n",
      "|    fps              | 11871    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41537    |\n",
      "----------------------------------\n",
      "Num timesteps: 41600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1872     |\n",
      "|    fps              | 11855    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41627    |\n",
      "----------------------------------\n",
      "Num timesteps: 41700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1876     |\n",
      "|    fps              | 11853    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41719    |\n",
      "----------------------------------\n",
      "Num timesteps: 41800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1880     |\n",
      "|    fps              | 11861    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41840    |\n",
      "----------------------------------\n",
      "Num timesteps: 41900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1884     |\n",
      "|    fps              | 11859    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 41932    |\n",
      "----------------------------------\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1888     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42018    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1892     |\n",
      "|    fps              | 11871    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42091    |\n",
      "----------------------------------\n",
      "Num timesteps: 42100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1896     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42162    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 42200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1900     |\n",
      "|    fps              | 11867    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42254    |\n",
      "----------------------------------\n",
      "Num timesteps: 42300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.79\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1904     |\n",
      "|    fps              | 11865    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42366    |\n",
      "----------------------------------\n",
      "Num timesteps: 42400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1908     |\n",
      "|    fps              | 11864    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42432    |\n",
      "----------------------------------\n",
      "Num timesteps: 42500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1912     |\n",
      "|    fps              | 11863    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42514    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1916     |\n",
      "|    fps              | 11873    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42597    |\n",
      "----------------------------------\n",
      "Num timesteps: 42600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.39\n",
      "Num timesteps: 42700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1920     |\n",
      "|    fps              | 11860    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42734    |\n",
      "----------------------------------\n",
      "Num timesteps: 42800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1924     |\n",
      "|    fps              | 11845    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42837    |\n",
      "----------------------------------\n",
      "Num timesteps: 42900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.41\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1928     |\n",
      "|    fps              | 11849    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 42938    |\n",
      "----------------------------------\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1932     |\n",
      "|    fps              | 11849    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43019    |\n",
      "----------------------------------\n",
      "Num timesteps: 43100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1936     |\n",
      "|    fps              | 11851    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43103    |\n",
      "----------------------------------\n",
      "Num timesteps: 43200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1940     |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43200    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1944     |\n",
      "|    fps              | 11844    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43265    |\n",
      "----------------------------------\n",
      "Num timesteps: 43300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1948     |\n",
      "|    fps              | 11838    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43349    |\n",
      "----------------------------------\n",
      "Num timesteps: 43400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1952     |\n",
      "|    fps              | 11842    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43476    |\n",
      "----------------------------------\n",
      "Num timesteps: 43500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1956     |\n",
      "|    fps              | 11845    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43548    |\n",
      "----------------------------------\n",
      "Num timesteps: 43600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1960     |\n",
      "|    fps              | 11844    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43650    |\n",
      "----------------------------------\n",
      "Num timesteps: 43700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1964     |\n",
      "|    fps              | 11846    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43730    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1968     |\n",
      "|    fps              | 11854    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43795    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 43800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1972     |\n",
      "|    fps              | 11847    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43871    |\n",
      "----------------------------------\n",
      "Num timesteps: 43900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1976     |\n",
      "|    fps              | 11853    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 43993    |\n",
      "----------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.74\n",
      "Num timesteps: 44100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1980     |\n",
      "|    fps              | 11849    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44126    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1984     |\n",
      "|    fps              | 11857    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44198    |\n",
      "----------------------------------\n",
      "Num timesteps: 44200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1988     |\n",
      "|    fps              | 11850    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44287    |\n",
      "----------------------------------\n",
      "Num timesteps: 44300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1992     |\n",
      "|    fps              | 11853    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44371    |\n",
      "----------------------------------\n",
      "Num timesteps: 44400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 1996     |\n",
      "|    fps              | 11834    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44451    |\n",
      "----------------------------------\n",
      "Num timesteps: 44500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2000     |\n",
      "|    fps              | 11839    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44556    |\n",
      "----------------------------------\n",
      "Num timesteps: 44600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2004     |\n",
      "|    fps              | 11832    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44609    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2008     |\n",
      "|    fps              | 11840    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44678    |\n",
      "----------------------------------\n",
      "Num timesteps: 44700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2012     |\n",
      "|    fps              | 11842    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44759    |\n",
      "----------------------------------\n",
      "Num timesteps: 44800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2016     |\n",
      "|    fps              | 11838    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44844    |\n",
      "----------------------------------\n",
      "Num timesteps: 44900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2020     |\n",
      "|    fps              | 11839    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 44932    |\n",
      "----------------------------------\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2024     |\n",
      "|    fps              | 11831    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45072    |\n",
      "----------------------------------\n",
      "Num timesteps: 45100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.3     |\n",
      "|    ep_rew_mean      | 22.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2028     |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45169    |\n",
      "----------------------------------\n",
      "Num timesteps: 45200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2032     |\n",
      "|    fps              | 11825    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45264    |\n",
      "----------------------------------\n",
      "Num timesteps: 45300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2036     |\n",
      "|    fps              | 11810    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45313    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2040     |\n",
      "|    fps              | 11816    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45368    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 45400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2044     |\n",
      "|    fps              | 11816    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45442    |\n",
      "----------------------------------\n",
      "Num timesteps: 45500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2048     |\n",
      "|    fps              | 11813    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45554    |\n",
      "----------------------------------\n",
      "Num timesteps: 45600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.46\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2052     |\n",
      "|    fps              | 11808    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45602    |\n",
      "----------------------------------\n",
      "Num timesteps: 45700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.8     |\n",
      "|    ep_rew_mean      | 21.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2056     |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45727    |\n",
      "----------------------------------\n",
      "Num timesteps: 45800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2060     |\n",
      "|    fps              | 11818    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45885    |\n",
      "----------------------------------\n",
      "Num timesteps: 45900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2064     |\n",
      "|    fps              | 11811    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 45944    |\n",
      "----------------------------------\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2068     |\n",
      "|    fps              | 11811    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46030    |\n",
      "----------------------------------\n",
      "Num timesteps: 46100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2072     |\n",
      "|    fps              | 11813    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46124    |\n",
      "----------------------------------\n",
      "Num timesteps: 46200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2076     |\n",
      "|    fps              | 11822    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46298    |\n",
      "----------------------------------\n",
      "Num timesteps: 46300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.05\n",
      "Num timesteps: 46400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.92\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2080     |\n",
      "|    fps              | 11818    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46419    |\n",
      "----------------------------------\n",
      "Num timesteps: 46500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.97\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2084     |\n",
      "|    fps              | 11819    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46507    |\n",
      "----------------------------------\n",
      "Num timesteps: 46600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2088     |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46604    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23       |\n",
      "|    ep_rew_mean      | 23       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2092     |\n",
      "|    fps              | 11828    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46675    |\n",
      "----------------------------------\n",
      "Num timesteps: 46700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2096     |\n",
      "|    fps              | 11824    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46736    |\n",
      "----------------------------------\n",
      "Num timesteps: 46800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2100     |\n",
      "|    fps              | 11821    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46801    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2104     |\n",
      "|    fps              | 11827    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 46873    |\n",
      "----------------------------------\n",
      "Num timesteps: 46900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.64\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2108     |\n",
      "|    fps              | 11824    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 47029    |\n",
      "----------------------------------\n",
      "Num timesteps: 47100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2112     |\n",
      "|    fps              | 11827    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 47144    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 47200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.77\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2116     |\n",
      "|    fps              | 11826    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 47216    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2120     |\n",
      "|    fps              | 11833    |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 47298    |\n",
      "----------------------------------\n",
      "Num timesteps: 47300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.66\n",
      "Num timesteps: 47400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.42\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2124     |\n",
      "|    fps              | 11823    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47417    |\n",
      "----------------------------------\n",
      "Num timesteps: 47500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.37\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2128     |\n",
      "|    fps              | 11824    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47519    |\n",
      "----------------------------------\n",
      "Num timesteps: 47600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2132     |\n",
      "|    fps              | 11828    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47622    |\n",
      "----------------------------------\n",
      "Num timesteps: 47700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2136     |\n",
      "|    fps              | 11830    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47725    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2140     |\n",
      "|    fps              | 11836    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47780    |\n",
      "----------------------------------\n",
      "Num timesteps: 47800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2144     |\n",
      "|    fps              | 11837    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47862    |\n",
      "----------------------------------\n",
      "Num timesteps: 47900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24       |\n",
      "|    ep_rew_mean      | 24       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2148     |\n",
      "|    fps              | 11832    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 47953    |\n",
      "----------------------------------\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 24.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2152     |\n",
      "|    fps              | 11836    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48065    |\n",
      "----------------------------------\n",
      "Num timesteps: 48100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.3     |\n",
      "|    ep_rew_mean      | 24.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2156     |\n",
      "|    fps              | 11831    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48154    |\n",
      "----------------------------------\n",
      "Num timesteps: 48200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2160     |\n",
      "|    fps              | 11833    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48243    |\n",
      "----------------------------------\n",
      "Num timesteps: 48300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2164     |\n",
      "|    fps              | 11817    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48305    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2168     |\n",
      "|    fps              | 11816    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48378    |\n",
      "----------------------------------\n",
      "Num timesteps: 48400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.56\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2172     |\n",
      "|    fps              | 11812    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48458    |\n",
      "----------------------------------\n",
      "Num timesteps: 48500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2176     |\n",
      "|    fps              | 11811    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48522    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2180     |\n",
      "|    fps              | 11816    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48584    |\n",
      "----------------------------------\n",
      "Num timesteps: 48600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.59\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2184     |\n",
      "|    fps              | 11810    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48659    |\n",
      "----------------------------------\n",
      "Num timesteps: 48700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.40\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2188     |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48716    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 48800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2192     |\n",
      "|    fps              | 11815    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48860    |\n",
      "----------------------------------\n",
      "Num timesteps: 48900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2196     |\n",
      "|    fps              | 11815    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 48936    |\n",
      "----------------------------------\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22       |\n",
      "|    ep_rew_mean      | 22       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2200     |\n",
      "|    fps              | 11809    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49001    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.1     |\n",
      "|    ep_rew_mean      | 22.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2204     |\n",
      "|    fps              | 11816    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49084    |\n",
      "----------------------------------\n",
      "Num timesteps: 49100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2208     |\n",
      "|    fps              | 11815    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49160    |\n",
      "----------------------------------\n",
      "Num timesteps: 49200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.99\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2212     |\n",
      "|    fps              | 11801    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49220    |\n",
      "----------------------------------\n",
      "Num timesteps: 49300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2216     |\n",
      "|    fps              | 11798    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49316    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.8     |\n",
      "|    ep_rew_mean      | 20.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2220     |\n",
      "|    fps              | 11803    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49373    |\n",
      "----------------------------------\n",
      "Num timesteps: 49400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.5     |\n",
      "|    ep_rew_mean      | 20.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2224     |\n",
      "|    fps              | 11805    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49471    |\n",
      "----------------------------------\n",
      "Num timesteps: 49500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2228     |\n",
      "|    fps              | 11805    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49554    |\n",
      "----------------------------------\n",
      "Num timesteps: 49600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2232     |\n",
      "|    fps              | 11804    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49644    |\n",
      "----------------------------------\n",
      "Num timesteps: 49700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 19.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2236     |\n",
      "|    fps              | 11803    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49715    |\n",
      "----------------------------------\n",
      "Num timesteps: 49800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2240     |\n",
      "|    fps              | 11798    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49804    |\n",
      "----------------------------------\n",
      "Num timesteps: 49900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2244     |\n",
      "|    fps              | 11805    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 49930    |\n",
      "----------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.6     |\n",
      "|    ep_rew_mean      | 20.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2248     |\n",
      "|    fps              | 11785    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50016    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.533    |\n",
      "|    n_updates        | 3        |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2252     |\n",
      "|    fps              | 11709    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50055    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.502    |\n",
      "|    n_updates        | 13       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2256     |\n",
      "|    fps              | 11639    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50092    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.492    |\n",
      "|    n_updates        | 22       |\n",
      "----------------------------------\n",
      "Num timesteps: 50100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 19.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2260     |\n",
      "|    fps              | 11546    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50129    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.468    |\n",
      "|    n_updates        | 32       |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.6     |\n",
      "|    ep_rew_mean      | 18.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2264     |\n",
      "|    fps              | 11479    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50164    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.454    |\n",
      "|    n_updates        | 40       |\n",
      "----------------------------------\n",
      "Num timesteps: 50200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 18.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2268     |\n",
      "|    fps              | 11382    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50205    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.434    |\n",
      "|    n_updates        | 51       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.9     |\n",
      "|    ep_rew_mean      | 17.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2272     |\n",
      "|    fps              | 11303    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50243    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.429    |\n",
      "|    n_updates        | 60       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2276     |\n",
      "|    fps              | 11216    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50290    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 72       |\n",
      "----------------------------------\n",
      "Num timesteps: 50300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 17.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.4     |\n",
      "|    ep_rew_mean      | 17.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2280     |\n",
      "|    fps              | 11140    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50326    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.37     |\n",
      "|    n_updates        | 81       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2284     |\n",
      "|    fps              | 11078    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50364    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 90       |\n",
      "----------------------------------\n",
      "Num timesteps: 50400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 16.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2288     |\n",
      "|    fps              | 10996    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50404    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.339    |\n",
      "|    n_updates        | 100      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2292     |\n",
      "|    fps              | 10928    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50443    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.315    |\n",
      "|    n_updates        | 110      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2296     |\n",
      "|    fps              | 10870    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50480    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.318    |\n",
      "|    n_updates        | 119      |\n",
      "----------------------------------\n",
      "Num timesteps: 50500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 15.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2300     |\n",
      "|    fps              | 10785    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50524    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.268    |\n",
      "|    n_updates        | 130      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2304     |\n",
      "|    fps              | 10721    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50564    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.255    |\n",
      "|    n_updates        | 140      |\n",
      "----------------------------------\n",
      "Num timesteps: 50600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 14.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2308     |\n",
      "|    fps              | 10642    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50606    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.257    |\n",
      "|    n_updates        | 151      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2312     |\n",
      "|    fps              | 10585    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50644    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 160      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2316     |\n",
      "|    fps              | 10524    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50683    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.208    |\n",
      "|    n_updates        | 170      |\n",
      "----------------------------------\n",
      "Num timesteps: 50700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 13.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.5     |\n",
      "|    ep_rew_mean      | 13.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2320     |\n",
      "|    fps              | 10441    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50723    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.18     |\n",
      "|    n_updates        | 180      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2324     |\n",
      "|    fps              | 10386    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50759    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.162    |\n",
      "|    n_updates        | 189      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.4     |\n",
      "|    ep_rew_mean      | 12.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2328     |\n",
      "|    fps              | 10328    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50797    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.147    |\n",
      "|    n_updates        | 199      |\n",
      "----------------------------------\n",
      "Num timesteps: 50800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 12.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2332     |\n",
      "|    fps              | 10265    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50835    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.126    |\n",
      "|    n_updates        | 208      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2336     |\n",
      "|    fps              | 10208    |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total_timesteps  | 50873    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.132    |\n",
      "|    n_updates        | 218      |\n",
      "----------------------------------\n",
      "Num timesteps: 50900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 11.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.1     |\n",
      "|    ep_rew_mean      | 11.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2340     |\n",
      "|    fps              | 10148    |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 50909    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.113    |\n",
      "|    n_updates        | 227      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.2     |\n",
      "|    ep_rew_mean      | 10.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2344     |\n",
      "|    fps              | 10093    |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 50950    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0988   |\n",
      "|    n_updates        | 237      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.71     |\n",
      "|    ep_rew_mean      | 9.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2348     |\n",
      "|    fps              | 10042    |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 50987    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.089    |\n",
      "|    n_updates        | 246      |\n",
      "----------------------------------\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.69     |\n",
      "|    ep_rew_mean      | 9.69     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2352     |\n",
      "|    fps              | 9985     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51024    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0546   |\n",
      "|    n_updates        | 255      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.72     |\n",
      "|    ep_rew_mean      | 9.72     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2356     |\n",
      "|    fps              | 9932     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51064    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0786   |\n",
      "|    n_updates        | 265      |\n",
      "----------------------------------\n",
      "Num timesteps: 51100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.72     |\n",
      "|    ep_rew_mean      | 9.72     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2360     |\n",
      "|    fps              | 9874     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51101    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0504   |\n",
      "|    n_updates        | 275      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.74     |\n",
      "|    ep_rew_mean      | 9.74     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2364     |\n",
      "|    fps              | 9827     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51138    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0521   |\n",
      "|    n_updates        | 284      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.69     |\n",
      "|    ep_rew_mean      | 9.69     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2368     |\n",
      "|    fps              | 9780     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51174    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.042    |\n",
      "|    n_updates        | 293      |\n",
      "----------------------------------\n",
      "Num timesteps: 51200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.71     |\n",
      "|    ep_rew_mean      | 9.71     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2372     |\n",
      "|    fps              | 9723     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51214    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0472   |\n",
      "|    n_updates        | 303      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.64     |\n",
      "|    ep_rew_mean      | 9.64     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2376     |\n",
      "|    fps              | 9675     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51254    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 313      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.65     |\n",
      "|    ep_rew_mean      | 9.65     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2380     |\n",
      "|    fps              | 9631     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51291    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 322      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 51300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.66     |\n",
      "|    ep_rew_mean      | 9.66     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2384     |\n",
      "|    fps              | 9575     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51330    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0262   |\n",
      "|    n_updates        | 332      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.62     |\n",
      "|    ep_rew_mean      | 9.62     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2388     |\n",
      "|    fps              | 9531     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51366    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0221   |\n",
      "|    n_updates        | 341      |\n",
      "----------------------------------\n",
      "Num timesteps: 51400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.6      |\n",
      "|    ep_rew_mean      | 9.6      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2392     |\n",
      "|    fps              | 9478     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51403    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0317   |\n",
      "|    n_updates        | 350      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.58     |\n",
      "|    ep_rew_mean      | 9.58     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2396     |\n",
      "|    fps              | 9434     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51438    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0216   |\n",
      "|    n_updates        | 359      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.52     |\n",
      "|    ep_rew_mean      | 9.52     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2400     |\n",
      "|    fps              | 9391     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0172   |\n",
      "|    n_updates        | 368      |\n",
      "----------------------------------\n",
      "Num timesteps: 51500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.53\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.53     |\n",
      "|    ep_rew_mean      | 9.53     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2404     |\n",
      "|    fps              | 9331     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51517    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 379      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.46     |\n",
      "|    ep_rew_mean      | 9.46     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2408     |\n",
      "|    fps              | 9294     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51552    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0206   |\n",
      "|    n_updates        | 387      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.47     |\n",
      "|    ep_rew_mean      | 9.47     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2412     |\n",
      "|    fps              | 9250     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51591    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0175   |\n",
      "|    n_updates        | 397      |\n",
      "----------------------------------\n",
      "Num timesteps: 51600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.72     |\n",
      "|    ep_rew_mean      | 9.72     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2416     |\n",
      "|    fps              | 9174     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 413      |\n",
      "----------------------------------\n",
      "Num timesteps: 51700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 9.72\n",
      "Num timesteps: 51800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 10.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2420     |\n",
      "|    fps              | 8966     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 51843    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.013    |\n",
      "|    n_updates        | 460      |\n",
      "----------------------------------\n",
      "Num timesteps: 51900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 11.20\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 11.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2424     |\n",
      "|    fps              | 8722     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total_timesteps  | 52085    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 521      |\n",
      "----------------------------------\n",
      "Num timesteps: 52100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 13.26\n",
      "Num timesteps: 52200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 13.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2428     |\n",
      "|    fps              | 8595     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52209    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00649  |\n",
      "|    n_updates        | 552      |\n",
      "----------------------------------\n",
      "Num timesteps: 52300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 14.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2432     |\n",
      "|    fps              | 8498     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52315    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00502  |\n",
      "|    n_updates        | 578      |\n",
      "----------------------------------\n",
      "Num timesteps: 52400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 15.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.5     |\n",
      "|    ep_rew_mean      | 15.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2436     |\n",
      "|    fps              | 8396     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52426    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00609  |\n",
      "|    n_updates        | 606      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 52500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 15.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2440     |\n",
      "|    fps              | 8298     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52536    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00647  |\n",
      "|    n_updates        | 633      |\n",
      "----------------------------------\n",
      "Num timesteps: 52600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 16.60\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2444     |\n",
      "|    fps              | 8203     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52637    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00478  |\n",
      "|    n_updates        | 659      |\n",
      "----------------------------------\n",
      "Num timesteps: 52700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 17.13\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.3     |\n",
      "|    ep_rew_mean      | 17.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2448     |\n",
      "|    fps              | 8129     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52721    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00521  |\n",
      "|    n_updates        | 680      |\n",
      "----------------------------------\n",
      "Num timesteps: 52800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 17.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.8     |\n",
      "|    ep_rew_mean      | 17.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2452     |\n",
      "|    fps              | 8061     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52804    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00313  |\n",
      "|    n_updates        | 700      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.3     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2456     |\n",
      "|    fps              | 7988     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52894    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00437  |\n",
      "|    n_updates        | 723      |\n",
      "----------------------------------\n",
      "Num timesteps: 52900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 18.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.9     |\n",
      "|    ep_rew_mean      | 18.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2460     |\n",
      "|    fps              | 7912     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 52986    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00278  |\n",
      "|    n_updates        | 746      |\n",
      "----------------------------------\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 18.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2464     |\n",
      "|    fps              | 7834     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 53083    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00524  |\n",
      "|    n_updates        | 770      |\n",
      "----------------------------------\n",
      "Num timesteps: 53100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 19.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2468     |\n",
      "|    fps              | 7745     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 53194    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00211  |\n",
      "|    n_updates        | 798      |\n",
      "----------------------------------\n",
      "Num timesteps: 53200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.20\n",
      "Num timesteps: 53300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 20.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21       |\n",
      "|    ep_rew_mean      | 21       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2472     |\n",
      "|    fps              | 7645     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 53315    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00204  |\n",
      "|    n_updates        | 828      |\n",
      "----------------------------------\n",
      "Num timesteps: 53400\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2476     |\n",
      "|    fps              | 7579     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53402    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00282  |\n",
      "|    n_updates        | 850      |\n",
      "----------------------------------\n",
      "Num timesteps: 53500\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 21.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2480     |\n",
      "|    fps              | 7502     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53509    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00118  |\n",
      "|    n_updates        | 877      |\n",
      "----------------------------------\n",
      "Num timesteps: 53600\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 22.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2484     |\n",
      "|    fps              | 7421     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53625    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00378  |\n",
      "|    n_updates        | 906      |\n",
      "----------------------------------\n",
      "Num timesteps: 53700\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.5     |\n",
      "|    ep_rew_mean      | 23.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2488     |\n",
      "|    fps              | 7361     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000752 |\n",
      "|    n_updates        | 927      |\n",
      "----------------------------------\n",
      "Num timesteps: 53800\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 23.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2492     |\n",
      "|    fps              | 7296     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000971 |\n",
      "|    n_updates        | 951      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2496     |\n",
      "|    fps              | 7246     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53890    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000645 |\n",
      "|    n_updates        | 972      |\n",
      "----------------------------------\n",
      "Num timesteps: 53900\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 24.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2500     |\n",
      "|    fps              | 7187     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 53978    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000453 |\n",
      "|    n_updates        | 994      |\n",
      "----------------------------------\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 25.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.4     |\n",
      "|    ep_rew_mean      | 25.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2504     |\n",
      "|    fps              | 7135     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54059    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00181  |\n",
      "|    n_updates        | 1014     |\n",
      "----------------------------------\n",
      "Num timesteps: 54100\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 25.65\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.9     |\n",
      "|    ep_rew_mean      | 25.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2508     |\n",
      "|    fps              | 7082     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54142    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000383 |\n",
      "|    n_updates        | 1035     |\n",
      "----------------------------------\n",
      "Num timesteps: 54200\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 26.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.4     |\n",
      "|    ep_rew_mean      | 26.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2512     |\n",
      "|    fps              | 7023     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54236    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000287 |\n",
      "|    n_updates        | 1058     |\n",
      "----------------------------------\n",
      "Num timesteps: 54300\n",
      "Best mean reward: 26.65 - Last mean reward per episode: 26.76\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.7     |\n",
      "|    ep_rew_mean      | 26.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2516     |\n",
      "|    fps              | 6959     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000885 |\n",
      "|    n_updates        | 1081     |\n",
      "----------------------------------\n",
      "Num timesteps: 54400\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 26.06\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 25.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2520     |\n",
      "|    fps              | 6921     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54414    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000643 |\n",
      "|    n_updates        | 1103     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.1     |\n",
      "|    ep_rew_mean      | 24.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2524     |\n",
      "|    fps              | 6895     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54493    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000254 |\n",
      "|    n_updates        | 1123     |\n",
      "----------------------------------\n",
      "Num timesteps: 54500\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 24.08\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2528     |\n",
      "|    fps              | 6855     |\n",
      "|    time_elapsed     | 7        |\n",
      "|    total_timesteps  | 54577    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000152 |\n",
      "|    n_updates        | 1144     |\n",
      "----------------------------------\n",
      "Num timesteps: 54600\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.4     |\n",
      "|    ep_rew_mean      | 23.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2532     |\n",
      "|    fps              | 6826     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 54654    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000234 |\n",
      "|    n_updates        | 1163     |\n",
      "----------------------------------\n",
      "Num timesteps: 54700\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.2     |\n",
      "|    ep_rew_mean      | 23.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2536     |\n",
      "|    fps              | 6793     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 54742    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00057  |\n",
      "|    n_updates        | 1185     |\n",
      "----------------------------------\n",
      "Num timesteps: 54800\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.96\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.8     |\n",
      "|    ep_rew_mean      | 22.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2540     |\n",
      "|    fps              | 6763     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 54820    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000126 |\n",
      "|    n_updates        | 1204     |\n",
      "----------------------------------\n",
      "Num timesteps: 54900\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2544     |\n",
      "|    fps              | 6723     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 54906    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000756 |\n",
      "|    n_updates        | 1226     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.7     |\n",
      "|    ep_rew_mean      | 22.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2548     |\n",
      "|    fps              | 6683     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 54988    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000848 |\n",
      "|    n_updates        | 1246     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 55000\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2552     |\n",
      "|    fps              | 6642     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55062    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000297 |\n",
      "|    n_updates        | 1265     |\n",
      "----------------------------------\n",
      "Num timesteps: 55100\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2556     |\n",
      "|    fps              | 6599     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55142    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000196 |\n",
      "|    n_updates        | 1285     |\n",
      "----------------------------------\n",
      "Num timesteps: 55200\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2560     |\n",
      "|    fps              | 6462     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55221    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000374 |\n",
      "|    n_updates        | 1305     |\n",
      "----------------------------------\n",
      "Num timesteps: 55300\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2564     |\n",
      "|    fps              | 6395     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55300    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000872 |\n",
      "|    n_updates        | 1324     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2568     |\n",
      "|    fps              | 6322     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55381    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000311 |\n",
      "|    n_updates        | 1345     |\n",
      "----------------------------------\n",
      "Num timesteps: 55400\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.87\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2572     |\n",
      "|    fps              | 6280     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55463    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000427 |\n",
      "|    n_updates        | 1365     |\n",
      "----------------------------------\n",
      "Num timesteps: 55500\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.6     |\n",
      "|    ep_rew_mean      | 21.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2576     |\n",
      "|    fps              | 6233     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55561    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000171 |\n",
      "|    n_updates        | 1390     |\n",
      "----------------------------------\n",
      "Num timesteps: 55600\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.55\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2580     |\n",
      "|    fps              | 6189     |\n",
      "|    time_elapsed     | 8        |\n",
      "|    total_timesteps  | 55649    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000227 |\n",
      "|    n_updates        | 1412     |\n",
      "----------------------------------\n",
      "Num timesteps: 55700\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.1     |\n",
      "|    ep_rew_mean      | 21.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2584     |\n",
      "|    fps              | 6153     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 55735    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000455 |\n",
      "|    n_updates        | 1433     |\n",
      "----------------------------------\n",
      "Num timesteps: 55800\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2588     |\n",
      "|    fps              | 6115     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 55828    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000514 |\n",
      "|    n_updates        | 1456     |\n",
      "----------------------------------\n",
      "Num timesteps: 55900\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.2     |\n",
      "|    ep_rew_mean      | 21.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2592     |\n",
      "|    fps              | 6068     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 55933    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.72e-05 |\n",
      "|    n_updates        | 1483     |\n",
      "----------------------------------\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2596     |\n",
      "|    fps              | 6028     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56024    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000983 |\n",
      "|    n_updates        | 1505     |\n",
      "----------------------------------\n",
      "Num timesteps: 56100\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2600     |\n",
      "|    fps              | 5988     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56111    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000112 |\n",
      "|    n_updates        | 1527     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2604     |\n",
      "|    fps              | 5953     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000691 |\n",
      "|    n_updates        | 1549     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 56200\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.38\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2608     |\n",
      "|    fps              | 5910     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56289    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000497 |\n",
      "|    n_updates        | 1572     |\n",
      "----------------------------------\n",
      "Num timesteps: 56300\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.47\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.5     |\n",
      "|    ep_rew_mean      | 21.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2612     |\n",
      "|    fps              | 5866     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56387    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000158 |\n",
      "|    n_updates        | 1596     |\n",
      "----------------------------------\n",
      "Num timesteps: 56400\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.7     |\n",
      "|    ep_rew_mean      | 21.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2616     |\n",
      "|    fps              | 5818     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56494    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000176 |\n",
      "|    n_updates        | 1623     |\n",
      "----------------------------------\n",
      "Num timesteps: 56500\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.66\n",
      "Num timesteps: 56600\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 21.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.9     |\n",
      "|    ep_rew_mean      | 21.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2620     |\n",
      "|    fps              | 5764     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56601    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.27e-05 |\n",
      "|    n_updates        | 1650     |\n",
      "----------------------------------\n",
      "Num timesteps: 56700\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.12\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2624     |\n",
      "|    fps              | 5714     |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 56711    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000312 |\n",
      "|    n_updates        | 1677     |\n",
      "----------------------------------\n",
      "Num timesteps: 56800\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2628     |\n",
      "|    fps              | 5674     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 56802    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000376 |\n",
      "|    n_updates        | 1700     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.4     |\n",
      "|    ep_rew_mean      | 22.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2632     |\n",
      "|    fps              | 5637     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 56898    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.73e-05 |\n",
      "|    n_updates        | 1724     |\n",
      "----------------------------------\n",
      "Num timesteps: 56900\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.44\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.6     |\n",
      "|    ep_rew_mean      | 22.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2636     |\n",
      "|    fps              | 5587     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57004    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.83e-05 |\n",
      "|    n_updates        | 1750     |\n",
      "----------------------------------\n",
      "Num timesteps: 57100\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 22.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2640     |\n",
      "|    fps              | 5543     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57112    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000113 |\n",
      "|    n_updates        | 1777     |\n",
      "----------------------------------\n",
      "Num timesteps: 57200\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.05\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2644     |\n",
      "|    fps              | 5504     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57213    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000234 |\n",
      "|    n_updates        | 1803     |\n",
      "----------------------------------\n",
      "Num timesteps: 57300\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.3     |\n",
      "|    ep_rew_mean      | 23.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2648     |\n",
      "|    fps              | 5463     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57319    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000365 |\n",
      "|    n_updates        | 1829     |\n",
      "----------------------------------\n",
      "Num timesteps: 57400\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.6     |\n",
      "|    ep_rew_mean      | 23.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2652     |\n",
      "|    fps              | 5424     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57427    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.95e-05 |\n",
      "|    n_updates        | 1856     |\n",
      "----------------------------------\n",
      "Num timesteps: 57500\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 23.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2656     |\n",
      "|    fps              | 5378     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57534    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000218 |\n",
      "|    n_updates        | 1883     |\n",
      "----------------------------------\n",
      "Num timesteps: 57600\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 24.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.2     |\n",
      "|    ep_rew_mean      | 24.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2660     |\n",
      "|    fps              | 5330     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57646    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.33e-05 |\n",
      "|    n_updates        | 1911     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 57700\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 24.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.6     |\n",
      "|    ep_rew_mean      | 24.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2664     |\n",
      "|    fps              | 5279     |\n",
      "|    time_elapsed     | 10       |\n",
      "|    total_timesteps  | 57760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000163 |\n",
      "|    n_updates        | 1939     |\n",
      "----------------------------------\n",
      "Num timesteps: 57800\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 24.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2668     |\n",
      "|    fps              | 5238     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 57874    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00037  |\n",
      "|    n_updates        | 1968     |\n",
      "----------------------------------\n",
      "Num timesteps: 57900\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 24.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.1     |\n",
      "|    ep_rew_mean      | 25.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2672     |\n",
      "|    fps              | 5204     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 57974    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000118 |\n",
      "|    n_updates        | 1993     |\n",
      "----------------------------------\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 25.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 25.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2676     |\n",
      "|    fps              | 5165     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58091    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000148 |\n",
      "|    n_updates        | 2022     |\n",
      "----------------------------------\n",
      "Num timesteps: 58100\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 25.30\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.5     |\n",
      "|    ep_rew_mean      | 25.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2680     |\n",
      "|    fps              | 5131     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58195    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000208 |\n",
      "|    n_updates        | 2048     |\n",
      "----------------------------------\n",
      "Num timesteps: 58200\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 25.46\n",
      "Num timesteps: 58300\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 25.78\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.8     |\n",
      "|    ep_rew_mean      | 25.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2684     |\n",
      "|    fps              | 5089     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 8.05e-05 |\n",
      "|    n_updates        | 2078     |\n",
      "----------------------------------\n",
      "Num timesteps: 58400\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 25.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 26.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2688     |\n",
      "|    fps              | 5048     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58445    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000164 |\n",
      "|    n_updates        | 2111     |\n",
      "----------------------------------\n",
      "Num timesteps: 58500\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 26.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 26.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2692     |\n",
      "|    fps              | 5009     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58555    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.65e-05 |\n",
      "|    n_updates        | 2138     |\n",
      "----------------------------------\n",
      "Num timesteps: 58600\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 26.26\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.5     |\n",
      "|    ep_rew_mean      | 26.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2696     |\n",
      "|    fps              | 4975     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58671    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000488 |\n",
      "|    n_updates        | 2167     |\n",
      "----------------------------------\n",
      "Num timesteps: 58700\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 26.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.7     |\n",
      "|    ep_rew_mean      | 26.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2700     |\n",
      "|    fps              | 4942     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58783    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.03e-05 |\n",
      "|    n_updates        | 2195     |\n",
      "----------------------------------\n",
      "Num timesteps: 58800\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 26.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27       |\n",
      "|    ep_rew_mean      | 27       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2704     |\n",
      "|    fps              | 4908     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 58899    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.6e-05  |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "Num timesteps: 58900\n",
      "Best mean reward: 26.76 - Last mean reward per episode: 27.02\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 27.02 - Last mean reward per episode: 27.10\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.2     |\n",
      "|    ep_rew_mean      | 27.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2708     |\n",
      "|    fps              | 4869     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59010    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00047  |\n",
      "|    n_updates        | 2252     |\n",
      "----------------------------------\n",
      "Num timesteps: 59100\n",
      "Best mean reward: 27.10 - Last mean reward per episode: 27.28\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.3     |\n",
      "|    ep_rew_mean      | 27.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2712     |\n",
      "|    fps              | 4836     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59114    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.69e-05 |\n",
      "|    n_updates        | 2278     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 59200\n",
      "Best mean reward: 27.28 - Last mean reward per episode: 27.30\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.3     |\n",
      "|    ep_rew_mean      | 27.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2716     |\n",
      "|    fps              | 4800     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59227    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.43e-05 |\n",
      "|    n_updates        | 2306     |\n",
      "----------------------------------\n",
      "Num timesteps: 59300\n",
      "Best mean reward: 27.30 - Last mean reward per episode: 27.42\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.4     |\n",
      "|    ep_rew_mean      | 27.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2720     |\n",
      "|    fps              | 4762     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59345    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000229 |\n",
      "|    n_updates        | 2336     |\n",
      "----------------------------------\n",
      "Num timesteps: 59400\n",
      "Best mean reward: 27.42 - Last mean reward per episode: 27.51\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.6     |\n",
      "|    ep_rew_mean      | 27.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2724     |\n",
      "|    fps              | 4726     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59471    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000439 |\n",
      "|    n_updates        | 2367     |\n",
      "----------------------------------\n",
      "Num timesteps: 59500\n",
      "Best mean reward: 27.51 - Last mean reward per episode: 27.56\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.9     |\n",
      "|    ep_rew_mean      | 27.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2728     |\n",
      "|    fps              | 4693     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59589    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000161 |\n",
      "|    n_updates        | 2397     |\n",
      "----------------------------------\n",
      "Num timesteps: 59600\n",
      "Best mean reward: 27.56 - Last mean reward per episode: 27.87\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 59700\n",
      "Best mean reward: 27.87 - Last mean reward per episode: 28.14\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.1     |\n",
      "|    ep_rew_mean      | 28.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2732     |\n",
      "|    fps              | 4655     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59705    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000152 |\n",
      "|    n_updates        | 2426     |\n",
      "----------------------------------\n",
      "Num timesteps: 59800\n",
      "Best mean reward: 28.14 - Last mean reward per episode: 28.23\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.4     |\n",
      "|    ep_rew_mean      | 28.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2736     |\n",
      "|    fps              | 4613     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 59848    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000227 |\n",
      "|    n_updates        | 2461     |\n",
      "----------------------------------\n",
      "Num timesteps: 59900\n",
      "Best mean reward: 28.23 - Last mean reward per episode: 28.45\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.5     |\n",
      "|    ep_rew_mean      | 28.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2740     |\n",
      "|    fps              | 4580     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 59965    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00108  |\n",
      "|    n_updates        | 2491     |\n",
      "----------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 28.45 - Last mean reward per episode: 28.57\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 60100\n",
      "Best mean reward: 28.57 - Last mean reward per episode: 28.89\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.2     |\n",
      "|    ep_rew_mean      | 29.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2744     |\n",
      "|    fps              | 4535     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60138    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0723   |\n",
      "|    n_updates        | 2534     |\n",
      "----------------------------------\n",
      "Num timesteps: 60200\n",
      "Best mean reward: 28.89 - Last mean reward per episode: 29.17\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29       |\n",
      "|    ep_rew_mean      | 29       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2748     |\n",
      "|    fps              | 4514     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60223    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0739   |\n",
      "|    n_updates        | 2555     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.7     |\n",
      "|    ep_rew_mean      | 28.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2752     |\n",
      "|    fps              | 4500     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0494   |\n",
      "|    n_updates        | 2573     |\n",
      "----------------------------------\n",
      "Num timesteps: 60300\n",
      "Best mean reward: 29.17 - Last mean reward per episode: 28.69\n",
      "Num timesteps: 60400\n",
      "Best mean reward: 29.17 - Last mean reward per episode: 28.71\n",
      "Num timesteps: 60500\n",
      "Best mean reward: 29.17 - Last mean reward per episode: 29.42\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30       |\n",
      "|    ep_rew_mean      | 30       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2756     |\n",
      "|    fps              | 4441     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60533    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0481   |\n",
      "|    n_updates        | 2633     |\n",
      "----------------------------------\n",
      "Num timesteps: 60600\n",
      "Best mean reward: 29.42 - Last mean reward per episode: 30.14\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 60700\n",
      "Best mean reward: 30.14 - Last mean reward per episode: 30.35\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30.6     |\n",
      "|    ep_rew_mean      | 30.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2760     |\n",
      "|    fps              | 4401     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60704    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0732   |\n",
      "|    n_updates        | 2675     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 60800\n",
      "Best mean reward: 30.35 - Last mean reward per episode: 30.75\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.2     |\n",
      "|    ep_rew_mean      | 31.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2764     |\n",
      "|    fps              | 4361     |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 60878    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0369   |\n",
      "|    n_updates        | 2719     |\n",
      "----------------------------------\n",
      "Num timesteps: 60900\n",
      "Best mean reward: 30.75 - Last mean reward per episode: 31.18\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.1     |\n",
      "|    ep_rew_mean      | 31.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2768     |\n",
      "|    fps              | 4335     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 60989    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0161   |\n",
      "|    n_updates        | 2747     |\n",
      "----------------------------------\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 31.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31       |\n",
      "|    ep_rew_mean      | 31       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2772     |\n",
      "|    fps              | 4317     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0203   |\n",
      "|    n_updates        | 2767     |\n",
      "----------------------------------\n",
      "Num timesteps: 61100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 30.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30.5     |\n",
      "|    ep_rew_mean      | 30.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2776     |\n",
      "|    fps              | 4301     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61141    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 2785     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 30       |\n",
      "|    ep_rew_mean      | 30       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2780     |\n",
      "|    fps              | 4289     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61197    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 2799     |\n",
      "----------------------------------\n",
      "Num timesteps: 61200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 30.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.4     |\n",
      "|    ep_rew_mean      | 29.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2784     |\n",
      "|    fps              | 4274     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61257    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0403   |\n",
      "|    n_updates        | 2814     |\n",
      "----------------------------------\n",
      "Num timesteps: 61300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 29.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.7     |\n",
      "|    ep_rew_mean      | 28.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2788     |\n",
      "|    fps              | 4260     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61318    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0349   |\n",
      "|    n_updates        | 2829     |\n",
      "----------------------------------\n",
      "Num timesteps: 61400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 28.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.5     |\n",
      "|    ep_rew_mean      | 28.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2792     |\n",
      "|    fps              | 4243     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61403    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 2850     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.2     |\n",
      "|    ep_rew_mean      | 28.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2796     |\n",
      "|    fps              | 4224     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61496    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0115   |\n",
      "|    n_updates        | 2873     |\n",
      "----------------------------------\n",
      "Num timesteps: 61500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 28.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.7     |\n",
      "|    ep_rew_mean      | 27.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2800     |\n",
      "|    fps              | 4211     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61550    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00346  |\n",
      "|    n_updates        | 2887     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.9     |\n",
      "|    ep_rew_mean      | 26.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2804     |\n",
      "|    fps              | 4203     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61593    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0362   |\n",
      "|    n_updates        | 2898     |\n",
      "----------------------------------\n",
      "Num timesteps: 61600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 26.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.4     |\n",
      "|    ep_rew_mean      | 26.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2808     |\n",
      "|    fps              | 4190     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61646    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00093  |\n",
      "|    n_updates        | 2911     |\n",
      "----------------------------------\n",
      "Num timesteps: 61700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 26.19\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.1     |\n",
      "|    ep_rew_mean      | 26.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2812     |\n",
      "|    fps              | 4170     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61726    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0478   |\n",
      "|    n_updates        | 2931     |\n",
      "----------------------------------\n",
      "Num timesteps: 61800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 25.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.7     |\n",
      "|    ep_rew_mean      | 25.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2816     |\n",
      "|    fps              | 4152     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61801    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00573  |\n",
      "|    n_updates        | 2950     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25.3     |\n",
      "|    ep_rew_mean      | 25.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2820     |\n",
      "|    fps              | 4134     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 61876    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 2968     |\n",
      "----------------------------------\n",
      "Num timesteps: 61900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 25.31\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 25       |\n",
      "|    ep_rew_mean      | 25       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2824     |\n",
      "|    fps              | 4116     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 61974    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0231   |\n",
      "|    n_updates        | 2993     |\n",
      "----------------------------------\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 25.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2828     |\n",
      "|    fps              | 4098     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62074    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 3018     |\n",
      "----------------------------------\n",
      "Num timesteps: 62100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 24.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.5     |\n",
      "|    ep_rew_mean      | 24.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2832     |\n",
      "|    fps              | 4083     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62151    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00135  |\n",
      "|    n_updates        | 3037     |\n",
      "----------------------------------\n",
      "Num timesteps: 62200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 24.14\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2836     |\n",
      "|    fps              | 4070     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62218    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 3054     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.1     |\n",
      "|    ep_rew_mean      | 23.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2840     |\n",
      "|    fps              | 4061     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62270    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 3067     |\n",
      "----------------------------------\n",
      "Num timesteps: 62300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 22.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.5     |\n",
      "|    ep_rew_mean      | 22.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2844     |\n",
      "|    fps              | 4041     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62388    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 3096     |\n",
      "----------------------------------\n",
      "Num timesteps: 62400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 22.50\n",
      "Num timesteps: 62500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 23.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.7     |\n",
      "|    ep_rew_mean      | 23.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2848     |\n",
      "|    fps              | 4000     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62594    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 3148     |\n",
      "----------------------------------\n",
      "Num timesteps: 62600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 23.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.9     |\n",
      "|    ep_rew_mean      | 23.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2852     |\n",
      "|    fps              | 3982     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62685    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0379   |\n",
      "|    n_updates        | 3171     |\n",
      "----------------------------------\n",
      "Num timesteps: 62700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 23.80\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.2     |\n",
      "|    ep_rew_mean      | 22.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2856     |\n",
      "|    fps              | 3971     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62751    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0307   |\n",
      "|    n_updates        | 3187     |\n",
      "----------------------------------\n",
      "Num timesteps: 62800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 21.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.3     |\n",
      "|    ep_rew_mean      | 21.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2860     |\n",
      "|    fps              | 3955     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62832    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0326   |\n",
      "|    n_updates        | 3207     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.2     |\n",
      "|    ep_rew_mean      | 20.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2864     |\n",
      "|    fps              | 3943     |\n",
      "|    time_elapsed     | 15       |\n",
      "|    total_timesteps  | 62898    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0463   |\n",
      "|    n_updates        | 3224     |\n",
      "----------------------------------\n",
      "Num timesteps: 62900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 20.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2868     |\n",
      "|    fps              | 3932     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 62956    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00598  |\n",
      "|    n_updates        | 3238     |\n",
      "----------------------------------\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2872     |\n",
      "|    fps              | 3916     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63036    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 3258     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 63100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2876     |\n",
      "|    fps              | 3900     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63110    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0096   |\n",
      "|    n_updates        | 3277     |\n",
      "----------------------------------\n",
      "Num timesteps: 63200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2880     |\n",
      "|    fps              | 3881     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63228    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 3306     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.4     |\n",
      "|    ep_rew_mean      | 20.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2884     |\n",
      "|    fps              | 3871     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63293    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00207  |\n",
      "|    n_updates        | 3323     |\n",
      "----------------------------------\n",
      "Num timesteps: 63300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 20.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.1     |\n",
      "|    ep_rew_mean      | 20.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2888     |\n",
      "|    fps              | 3864     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63329    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00943  |\n",
      "|    n_updates        | 3332     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.6     |\n",
      "|    ep_rew_mean      | 19.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2892     |\n",
      "|    fps              | 3859     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63364    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0139   |\n",
      "|    n_updates        | 3340     |\n",
      "----------------------------------\n",
      "Num timesteps: 63400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2896     |\n",
      "|    fps              | 3848     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63405    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00289  |\n",
      "|    n_updates        | 3351     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2900     |\n",
      "|    fps              | 3837     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63470    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0341   |\n",
      "|    n_updates        | 3367     |\n",
      "----------------------------------\n",
      "Num timesteps: 63500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.20\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.7     |\n",
      "|    ep_rew_mean      | 19.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2904     |\n",
      "|    fps              | 3823     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63559    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00505  |\n",
      "|    n_updates        | 3389     |\n",
      "----------------------------------\n",
      "Num timesteps: 63600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.5     |\n",
      "|    ep_rew_mean      | 19.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2908     |\n",
      "|    fps              | 3815     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63600    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0469   |\n",
      "|    n_updates        | 3399     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.1     |\n",
      "|    ep_rew_mean      | 19.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2912     |\n",
      "|    fps              | 3809     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63638    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 3409     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.7     |\n",
      "|    ep_rew_mean      | 18.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2916     |\n",
      "|    fps              | 3803     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63675    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 3418     |\n",
      "----------------------------------\n",
      "Num timesteps: 63700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 18.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.4     |\n",
      "|    ep_rew_mean      | 18.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2920     |\n",
      "|    fps              | 3791     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 3427     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.7     |\n",
      "|    ep_rew_mean      | 17.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2924     |\n",
      "|    fps              | 3784     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63748    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 3436     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.2     |\n",
      "|    ep_rew_mean      | 17.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2928     |\n",
      "|    fps              | 3777     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63790    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00418  |\n",
      "|    n_updates        | 3447     |\n",
      "----------------------------------\n",
      "Num timesteps: 63800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 17.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 17.1     |\n",
      "|    ep_rew_mean      | 17.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2932     |\n",
      "|    fps              | 3758     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 63856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0556   |\n",
      "|    n_updates        | 3463     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 63900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 17.05\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 18.22\n",
      "Num timesteps: 64100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 18.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.4     |\n",
      "|    ep_rew_mean      | 19.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2936     |\n",
      "|    fps              | 3696     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64157    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0019   |\n",
      "|    n_updates        | 3539     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.2     |\n",
      "|    ep_rew_mean      | 19.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2940     |\n",
      "|    fps              | 3690     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64194    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0228   |\n",
      "|    n_updates        | 3548     |\n",
      "----------------------------------\n",
      "Num timesteps: 64200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.5     |\n",
      "|    ep_rew_mean      | 18.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2944     |\n",
      "|    fps              | 3683     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64234    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0473   |\n",
      "|    n_updates        | 3558     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2948     |\n",
      "|    fps              | 3678     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64275    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0133   |\n",
      "|    n_updates        | 3568     |\n",
      "----------------------------------\n",
      "Num timesteps: 64300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.3     |\n",
      "|    ep_rew_mean      | 16.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2952     |\n",
      "|    fps              | 3670     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64318    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.037    |\n",
      "|    n_updates        | 3579     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2956     |\n",
      "|    fps              | 3664     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64354    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0276   |\n",
      "|    n_updates        | 3588     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2960     |\n",
      "|    fps              | 3659     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64395    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 3598     |\n",
      "----------------------------------\n",
      "Num timesteps: 64400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.63\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2964     |\n",
      "|    fps              | 3652     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64435    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00127  |\n",
      "|    n_updates        | 3608     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2968     |\n",
      "|    fps              | 3646     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64475    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 3618     |\n",
      "----------------------------------\n",
      "Num timesteps: 64500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.95\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.8     |\n",
      "|    ep_rew_mean      | 14.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2972     |\n",
      "|    fps              | 3640     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64514    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0544   |\n",
      "|    n_updates        | 3628     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2976     |\n",
      "|    fps              | 3634     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64553    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00242  |\n",
      "|    n_updates        | 3638     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2980     |\n",
      "|    fps              | 3630     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64587    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.021    |\n",
      "|    n_updates        | 3646     |\n",
      "----------------------------------\n",
      "Num timesteps: 64600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.45\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2984     |\n",
      "|    fps              | 3624     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64624    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0038   |\n",
      "|    n_updates        | 3655     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.3     |\n",
      "|    ep_rew_mean      | 13.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2988     |\n",
      "|    fps              | 3618     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64664    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 3665     |\n",
      "----------------------------------\n",
      "Num timesteps: 64700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.43\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2992     |\n",
      "|    fps              | 3601     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 64793    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0381   |\n",
      "|    n_updates        | 3698     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 64800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.29\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 2996     |\n",
      "|    fps              | 3594     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 64837    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 3709     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.1     |\n",
      "|    ep_rew_mean      | 14.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3000     |\n",
      "|    fps              | 3588     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 64876    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00393  |\n",
      "|    n_updates        | 3718     |\n",
      "----------------------------------\n",
      "Num timesteps: 64900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3004     |\n",
      "|    fps              | 3582     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 64915    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 3728     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3008     |\n",
      "|    fps              | 3577     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 64956    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 3738     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3012     |\n",
      "|    fps              | 3571     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 64996    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0402   |\n",
      "|    n_updates        | 3748     |\n",
      "----------------------------------\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3016     |\n",
      "|    fps              | 3565     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65031    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0226   |\n",
      "|    n_updates        | 3757     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3020     |\n",
      "|    fps              | 3560     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65079    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0198   |\n",
      "|    n_updates        | 3769     |\n",
      "----------------------------------\n",
      "Num timesteps: 65100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.68\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.7     |\n",
      "|    ep_rew_mean      | 13.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3024     |\n",
      "|    fps              | 3553     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65121    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00614  |\n",
      "|    n_updates        | 3780     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3028     |\n",
      "|    fps              | 3547     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65165    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00238  |\n",
      "|    n_updates        | 3791     |\n",
      "----------------------------------\n",
      "Num timesteps: 65200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.6     |\n",
      "|    ep_rew_mean      | 13.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3032     |\n",
      "|    fps              | 3540     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65211    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0248   |\n",
      "|    n_updates        | 3802     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3036     |\n",
      "|    fps              | 3536     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65250    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0408   |\n",
      "|    n_updates        | 3812     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3040     |\n",
      "|    fps              | 3531     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65285    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00129  |\n",
      "|    n_updates        | 3821     |\n",
      "----------------------------------\n",
      "Num timesteps: 65300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.90\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3044     |\n",
      "|    fps              | 3526     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65323    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0221   |\n",
      "|    n_updates        | 3830     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3048     |\n",
      "|    fps              | 3521     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65358    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0263   |\n",
      "|    n_updates        | 3839     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3052     |\n",
      "|    fps              | 3517     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65394    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 3848     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 65400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3056     |\n",
      "|    fps              | 3511     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65433    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0151   |\n",
      "|    n_updates        | 3858     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3060     |\n",
      "|    fps              | 3507     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65468    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 3866     |\n",
      "----------------------------------\n",
      "Num timesteps: 65500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.70\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3064     |\n",
      "|    fps              | 3501     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65503    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0238   |\n",
      "|    n_updates        | 3875     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.6     |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3068     |\n",
      "|    fps              | 3497     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65538    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00301  |\n",
      "|    n_updates        | 3884     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.6     |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3072     |\n",
      "|    fps              | 3492     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65574    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0271   |\n",
      "|    n_updates        | 3893     |\n",
      "----------------------------------\n",
      "Num timesteps: 65600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.6     |\n",
      "|    ep_rew_mean      | 10.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3076     |\n",
      "|    fps              | 3486     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65616    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0353   |\n",
      "|    n_updates        | 3903     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.7     |\n",
      "|    ep_rew_mean      | 10.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3080     |\n",
      "|    fps              | 3481     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65658    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 3914     |\n",
      "----------------------------------\n",
      "Num timesteps: 65700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.72\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3084     |\n",
      "|    fps              | 3475     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 65702    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 3925     |\n",
      "----------------------------------\n",
      "Num timesteps: 65800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.6     |\n",
      "|    ep_rew_mean      | 11.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3088     |\n",
      "|    fps              | 3460     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 65827    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00884  |\n",
      "|    n_updates        | 3956     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3092     |\n",
      "|    fps              | 3454     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 65887    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00519  |\n",
      "|    n_updates        | 3971     |\n",
      "----------------------------------\n",
      "Num timesteps: 65900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3096     |\n",
      "|    fps              | 3448     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 65924    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0455   |\n",
      "|    n_updates        | 3980     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3100     |\n",
      "|    fps              | 3444     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 65960    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00362  |\n",
      "|    n_updates        | 3989     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3104     |\n",
      "|    fps              | 3440     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 65999    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 3999     |\n",
      "----------------------------------\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.84\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3108     |\n",
      "|    fps              | 3435     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66039    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 4009     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.8     |\n",
      "|    ep_rew_mean      | 10.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3112     |\n",
      "|    fps              | 3430     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66080    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0363   |\n",
      "|    n_updates        | 4019     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 66100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 10.85\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.9     |\n",
      "|    ep_rew_mean      | 10.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3116     |\n",
      "|    fps              | 3425     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66118    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 4029     |\n",
      "----------------------------------\n",
      "Num timesteps: 66200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.34\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.2     |\n",
      "|    ep_rew_mean      | 11.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3120     |\n",
      "|    fps              | 3414     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66202    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00221  |\n",
      "|    n_updates        | 4050     |\n",
      "----------------------------------\n",
      "Num timesteps: 66300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3124     |\n",
      "|    fps              | 3396     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66346    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 4086     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3128     |\n",
      "|    fps              | 3392     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66385    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 4096     |\n",
      "----------------------------------\n",
      "Num timesteps: 66400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3132     |\n",
      "|    fps              | 3387     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66419    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00154  |\n",
      "|    n_updates        | 4104     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3136     |\n",
      "|    fps              | 3383     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66456    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00893  |\n",
      "|    n_updates        | 4113     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3140     |\n",
      "|    fps              | 3379     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66495    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00481  |\n",
      "|    n_updates        | 4123     |\n",
      "----------------------------------\n",
      "Num timesteps: 66500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3144     |\n",
      "|    fps              | 3374     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66533    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 4133     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.1     |\n",
      "|    ep_rew_mean      | 12.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3148     |\n",
      "|    fps              | 3370     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66571    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00273  |\n",
      "|    n_updates        | 4142     |\n",
      "----------------------------------\n",
      "Num timesteps: 66600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.16\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.2     |\n",
      "|    ep_rew_mean      | 12.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3152     |\n",
      "|    fps              | 3365     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66610    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0404   |\n",
      "|    n_updates        | 4152     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.3     |\n",
      "|    ep_rew_mean      | 12.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3156     |\n",
      "|    fps              | 3360     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 66659    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.031    |\n",
      "|    n_updates        | 4164     |\n",
      "----------------------------------\n",
      "Num timesteps: 66700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.26\n",
      "Num timesteps: 66800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.88\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3160     |\n",
      "|    fps              | 3339     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 66846    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00372  |\n",
      "|    n_updates        | 4211     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.8     |\n",
      "|    ep_rew_mean      | 13.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3164     |\n",
      "|    fps              | 3335     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 66884    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0084   |\n",
      "|    n_updates        | 4220     |\n",
      "----------------------------------\n",
      "Num timesteps: 66900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3168     |\n",
      "|    fps              | 3326     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 66962    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0356   |\n",
      "|    n_updates        | 4240     |\n",
      "----------------------------------\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3172     |\n",
      "|    fps              | 3321     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67002    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00305  |\n",
      "|    n_updates        | 4250     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3176     |\n",
      "|    fps              | 3317     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67039    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00136  |\n",
      "|    n_updates        | 4259     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3180     |\n",
      "|    fps              | 3312     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67075    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000999 |\n",
      "|    n_updates        | 4268     |\n",
      "----------------------------------\n",
      "Num timesteps: 67100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.17\n",
      "Num timesteps: 67200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.10\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3184     |\n",
      "|    fps              | 3294     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67234    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.034    |\n",
      "|    n_updates        | 4308     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.5     |\n",
      "|    ep_rew_mean      | 14.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3188     |\n",
      "|    fps              | 3290     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67273    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.017    |\n",
      "|    n_updates        | 4318     |\n",
      "----------------------------------\n",
      "Num timesteps: 67300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.25\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3192     |\n",
      "|    fps              | 3285     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67313    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0351   |\n",
      "|    n_updates        | 4328     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3196     |\n",
      "|    fps              | 3281     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67354    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0121   |\n",
      "|    n_updates        | 4338     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.3     |\n",
      "|    ep_rew_mean      | 14.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3200     |\n",
      "|    fps              | 3277     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67393    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0222   |\n",
      "|    n_updates        | 4348     |\n",
      "----------------------------------\n",
      "Num timesteps: 67400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.33\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3204     |\n",
      "|    fps              | 3272     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67435    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 4358     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3208     |\n",
      "|    fps              | 3269     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 4368     |\n",
      "----------------------------------\n",
      "Num timesteps: 67500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.36\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3212     |\n",
      "|    fps              | 3262     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67541    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0208   |\n",
      "|    n_updates        | 4385     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3216     |\n",
      "|    fps              | 3258     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67586    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0294   |\n",
      "|    n_updates        | 4396     |\n",
      "----------------------------------\n",
      "Num timesteps: 67600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3220     |\n",
      "|    fps              | 3253     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67623    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00962  |\n",
      "|    n_updates        | 4405     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3224     |\n",
      "|    fps              | 3250     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67661    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0173   |\n",
      "|    n_updates        | 4415     |\n",
      "----------------------------------\n",
      "Num timesteps: 67700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3228     |\n",
      "|    fps              | 3245     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67700    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0102   |\n",
      "|    n_updates        | 4424     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3232     |\n",
      "|    fps              | 3241     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67737    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0236   |\n",
      "|    n_updates        | 4434     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3236     |\n",
      "|    fps              | 3237     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67774    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 4443     |\n",
      "----------------------------------\n",
      "Num timesteps: 67800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3240     |\n",
      "|    fps              | 3233     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 67811    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 4452     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3244     |\n",
      "|    fps              | 3229     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 67855    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 4463     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13.2     |\n",
      "|    ep_rew_mean      | 13.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3248     |\n",
      "|    fps              | 3226     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 67893    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0459   |\n",
      "|    n_updates        | 4473     |\n",
      "----------------------------------\n",
      "Num timesteps: 67900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.22\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.4     |\n",
      "|    ep_rew_mean      | 14.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3252     |\n",
      "|    fps              | 3210     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68048    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00966  |\n",
      "|    n_updates        | 4511     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.2     |\n",
      "|    ep_rew_mean      | 14.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3256     |\n",
      "|    fps              | 3206     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68083    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0376   |\n",
      "|    n_updates        | 4520     |\n",
      "----------------------------------\n",
      "Num timesteps: 68100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.7     |\n",
      "|    ep_rew_mean      | 12.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3260     |\n",
      "|    fps              | 3201     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.011    |\n",
      "|    n_updates        | 4529     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.8     |\n",
      "|    ep_rew_mean      | 12.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3264     |\n",
      "|    fps              | 3196     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68165    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0303   |\n",
      "|    n_updates        | 4541     |\n",
      "----------------------------------\n",
      "Num timesteps: 68200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.81\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3268     |\n",
      "|    fps              | 3186     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68253    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0103   |\n",
      "|    n_updates        | 4563     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3272     |\n",
      "|    fps              | 3182     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68296    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00591  |\n",
      "|    n_updates        | 4573     |\n",
      "----------------------------------\n",
      "Num timesteps: 68300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.94\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 12.9     |\n",
      "|    ep_rew_mean      | 12.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3276     |\n",
      "|    fps              | 3177     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68332    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 4582     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3280     |\n",
      "|    fps              | 3172     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68371    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 4592     |\n",
      "----------------------------------\n",
      "Num timesteps: 68400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3284     |\n",
      "|    fps              | 3167     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68408    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00443  |\n",
      "|    n_updates        | 4601     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.7     |\n",
      "|    ep_rew_mean      | 11.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3288     |\n",
      "|    fps              | 3163     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68447    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 4611     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 11.9     |\n",
      "|    ep_rew_mean      | 11.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3292     |\n",
      "|    fps              | 3158     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68499    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 4624     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 68500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.86\n",
      "Num timesteps: 68600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 11.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3296     |\n",
      "|    fps              | 3142     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 68654    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0286   |\n",
      "|    n_updates        | 4663     |\n",
      "----------------------------------\n",
      "Num timesteps: 68700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.00\n",
      "Num timesteps: 68800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3300     |\n",
      "|    fps              | 3122     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 68856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00389  |\n",
      "|    n_updates        | 4713     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3304     |\n",
      "|    fps              | 3118     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 68899    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 4724     |\n",
      "----------------------------------\n",
      "Num timesteps: 68900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.9     |\n",
      "|    ep_rew_mean      | 14.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3308     |\n",
      "|    fps              | 3111     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 68971    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 4742     |\n",
      "----------------------------------\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3312     |\n",
      "|    fps              | 3107     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69009    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0193   |\n",
      "|    n_updates        | 4752     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.6     |\n",
      "|    ep_rew_mean      | 14.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3316     |\n",
      "|    fps              | 3104     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69049    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00457  |\n",
      "|    n_updates        | 4762     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3320     |\n",
      "|    fps              | 3101     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69090    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0424   |\n",
      "|    n_updates        | 4772     |\n",
      "----------------------------------\n",
      "Num timesteps: 69100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.66\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3324     |\n",
      "|    fps              | 3097     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69133    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0522   |\n",
      "|    n_updates        | 4783     |\n",
      "----------------------------------\n",
      "Num timesteps: 69200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.3     |\n",
      "|    ep_rew_mean      | 15.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3328     |\n",
      "|    fps              | 3088     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69233    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00239  |\n",
      "|    n_updates        | 4808     |\n",
      "----------------------------------\n",
      "Num timesteps: 69300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.61\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.1     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3332     |\n",
      "|    fps              | 3079     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69345    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 4836     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.2     |\n",
      "|    ep_rew_mean      | 16.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3336     |\n",
      "|    fps              | 3074     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69397    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 4849     |\n",
      "----------------------------------\n",
      "Num timesteps: 69400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.23\n",
      "Num timesteps: 69500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.76\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3340     |\n",
      "|    fps              | 3064     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69502    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0347   |\n",
      "|    n_updates        | 4875     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3344     |\n",
      "|    fps              | 3060     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69540    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00598  |\n",
      "|    n_updates        | 4884     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.9     |\n",
      "|    ep_rew_mean      | 16.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3348     |\n",
      "|    fps              | 3056     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69578    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0678   |\n",
      "|    n_updates        | 4894     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 69600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.71\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3352     |\n",
      "|    fps              | 3049     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69618    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00199  |\n",
      "|    n_updates        | 4904     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3356     |\n",
      "|    fps              | 3044     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69655    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00895  |\n",
      "|    n_updates        | 4913     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.7     |\n",
      "|    ep_rew_mean      | 15.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3360     |\n",
      "|    fps              | 3040     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69690    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 4922     |\n",
      "----------------------------------\n",
      "Num timesteps: 69700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.67\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.6     |\n",
      "|    ep_rew_mean      | 15.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3364     |\n",
      "|    fps              | 3037     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69725    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 4931     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.1     |\n",
      "|    ep_rew_mean      | 15.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3368     |\n",
      "|    fps              | 3033     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 69760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.012    |\n",
      "|    n_updates        | 4939     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3372     |\n",
      "|    fps              | 3030     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 69799    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00331  |\n",
      "|    n_updates        | 4949     |\n",
      "----------------------------------\n",
      "Num timesteps: 69800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.2     |\n",
      "|    ep_rew_mean      | 15.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3376     |\n",
      "|    fps              | 3024     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 69853    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00167  |\n",
      "|    n_updates        | 4963     |\n",
      "----------------------------------\n",
      "Num timesteps: 69900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.21\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3380     |\n",
      "|    fps              | 3008     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 69963    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00147  |\n",
      "|    n_updates        | 4990     |\n",
      "----------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.93\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3384     |\n",
      "|    fps              | 3002     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70002    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.46     |\n",
      "|    n_updates        | 5000     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.9     |\n",
      "|    ep_rew_mean      | 15.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3388     |\n",
      "|    fps              | 2999     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70040    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.287    |\n",
      "|    n_updates        | 5009     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.8     |\n",
      "|    ep_rew_mean      | 15.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3392     |\n",
      "|    fps              | 2995     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70084    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0828   |\n",
      "|    n_updates        | 5020     |\n",
      "----------------------------------\n",
      "Num timesteps: 70100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.86\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14.7     |\n",
      "|    ep_rew_mean      | 14.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3396     |\n",
      "|    fps              | 2989     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70122    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00604  |\n",
      "|    n_updates        | 5030     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3400     |\n",
      "|    fps              | 2983     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70159    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0445   |\n",
      "|    n_updates        | 5039     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 13       |\n",
      "|    ep_rew_mean      | 13       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3404     |\n",
      "|    fps              | 2980     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70198    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0819   |\n",
      "|    n_updates        | 5049     |\n",
      "----------------------------------\n",
      "Num timesteps: 70200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 12.99\n",
      "Num timesteps: 70300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 13.22\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 14       |\n",
      "|    ep_rew_mean      | 14       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3408     |\n",
      "|    fps              | 2946     |\n",
      "|    time_elapsed     | 23       |\n",
      "|    total_timesteps  | 70374    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0299   |\n",
      "|    n_updates        | 5093     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 70400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.03\n",
      "Num timesteps: 70500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 14.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15       |\n",
      "|    ep_rew_mean      | 15       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3412     |\n",
      "|    fps              | 2921     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70510    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0934   |\n",
      "|    n_updates        | 5127     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 15.4     |\n",
      "|    ep_rew_mean      | 15.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3416     |\n",
      "|    fps              | 2914     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70593    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 5148     |\n",
      "----------------------------------\n",
      "Num timesteps: 70600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.44\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16       |\n",
      "|    ep_rew_mean      | 16       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3420     |\n",
      "|    fps              | 2906     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70688    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0459   |\n",
      "|    n_updates        | 5171     |\n",
      "----------------------------------\n",
      "Num timesteps: 70700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 15.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.6     |\n",
      "|    ep_rew_mean      | 16.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3424     |\n",
      "|    fps              | 2896     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70797    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.049    |\n",
      "|    n_updates        | 5199     |\n",
      "----------------------------------\n",
      "Num timesteps: 70800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.5     |\n",
      "|    ep_rew_mean      | 16.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3428     |\n",
      "|    fps              | 2889     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70885    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.047    |\n",
      "|    n_updates        | 5221     |\n",
      "----------------------------------\n",
      "Num timesteps: 70900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.52\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.1     |\n",
      "|    ep_rew_mean      | 16.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3432     |\n",
      "|    fps              | 2884     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 70954    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.058    |\n",
      "|    n_updates        | 5238     |\n",
      "----------------------------------\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 16.8     |\n",
      "|    ep_rew_mean      | 16.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3436     |\n",
      "|    fps              | 2876     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 71072    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0808   |\n",
      "|    n_updates        | 5267     |\n",
      "----------------------------------\n",
      "Num timesteps: 71100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 16.75\n",
      "Num timesteps: 71200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 17.55\n",
      "Num timesteps: 71300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 18.11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 18.2     |\n",
      "|    ep_rew_mean      | 18.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3440     |\n",
      "|    fps              | 2857     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total_timesteps  | 71319    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0282   |\n",
      "|    n_updates        | 5329     |\n",
      "----------------------------------\n",
      "Num timesteps: 71400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 18.58\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19       |\n",
      "|    ep_rew_mean      | 19       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3444     |\n",
      "|    fps              | 2849     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 71441    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.12     |\n",
      "|    n_updates        | 5360     |\n",
      "----------------------------------\n",
      "Num timesteps: 71500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 19.35\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 19.9     |\n",
      "|    ep_rew_mean      | 19.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3448     |\n",
      "|    fps              | 2840     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 71567    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0621   |\n",
      "|    n_updates        | 5391     |\n",
      "----------------------------------\n",
      "Num timesteps: 71600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 20.07\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.3     |\n",
      "|    ep_rew_mean      | 20.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3452     |\n",
      "|    fps              | 2834     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 71647    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0917   |\n",
      "|    n_updates        | 5411     |\n",
      "----------------------------------\n",
      "Num timesteps: 71700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 20.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 20.7     |\n",
      "|    ep_rew_mean      | 20.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3456     |\n",
      "|    fps              | 2827     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 71721    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.034    |\n",
      "|    n_updates        | 5430     |\n",
      "----------------------------------\n",
      "Num timesteps: 71800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 21.15\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 21.4     |\n",
      "|    ep_rew_mean      | 21.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3460     |\n",
      "|    fps              | 2817     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 71835    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0432   |\n",
      "|    n_updates        | 5458     |\n",
      "----------------------------------\n",
      "Num timesteps: 71900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 21.45\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 22.73\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 22.9     |\n",
      "|    ep_rew_mean      | 22.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3464     |\n",
      "|    fps              | 2803     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 72018    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0154   |\n",
      "|    n_updates        | 5504     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 72100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 23.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 23.8     |\n",
      "|    ep_rew_mean      | 23.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3468     |\n",
      "|    fps              | 2789     |\n",
      "|    time_elapsed     | 25       |\n",
      "|    total_timesteps  | 72143    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0104   |\n",
      "|    n_updates        | 5535     |\n",
      "----------------------------------\n",
      "Num timesteps: 72200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 23.83\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 24.9     |\n",
      "|    ep_rew_mean      | 24.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3472     |\n",
      "|    fps              | 2780     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72294    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0833   |\n",
      "|    n_updates        | 5573     |\n",
      "----------------------------------\n",
      "Num timesteps: 72300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 24.95\n",
      "Num timesteps: 72400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 25.50\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 26.2     |\n",
      "|    ep_rew_mean      | 26.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3476     |\n",
      "|    fps              | 2764     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72476    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0505   |\n",
      "|    n_updates        | 5618     |\n",
      "----------------------------------\n",
      "Num timesteps: 72500\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 26.23\n",
      "Num timesteps: 72600\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 26.37\n",
      "Num timesteps: 72700\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 27.00\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.5     |\n",
      "|    ep_rew_mean      | 27.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3480     |\n",
      "|    fps              | 2746     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72712    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00246  |\n",
      "|    n_updates        | 5677     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 27.9     |\n",
      "|    ep_rew_mean      | 27.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3484     |\n",
      "|    fps              | 2741     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72793    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 5698     |\n",
      "----------------------------------\n",
      "Num timesteps: 72800\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 27.91\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 28.4     |\n",
      "|    ep_rew_mean      | 28.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3488     |\n",
      "|    fps              | 2735     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72879    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0571   |\n",
      "|    n_updates        | 5719     |\n",
      "----------------------------------\n",
      "Num timesteps: 72900\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 28.39\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29       |\n",
      "|    ep_rew_mean      | 29       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3492     |\n",
      "|    fps              | 2727     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 72982    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.129    |\n",
      "|    n_updates        | 5745     |\n",
      "----------------------------------\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 28.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.4     |\n",
      "|    ep_rew_mean      | 29.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3496     |\n",
      "|    fps              | 2721     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 73058    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0833   |\n",
      "|    n_updates        | 5764     |\n",
      "----------------------------------\n",
      "Num timesteps: 73100\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 29.54\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 29.9     |\n",
      "|    ep_rew_mean      | 29.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3500     |\n",
      "|    fps              | 2714     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total_timesteps  | 73149    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0903   |\n",
      "|    n_updates        | 5787     |\n",
      "----------------------------------\n",
      "Num timesteps: 73200\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 30.12\n",
      "Num timesteps: 73300\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 30.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 31.8     |\n",
      "|    ep_rew_mean      | 31.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3504     |\n",
      "|    fps              | 2697     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 73381    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0114   |\n",
      "|    n_updates        | 5845     |\n",
      "----------------------------------\n",
      "Num timesteps: 73400\n",
      "Best mean reward: 31.18 - Last mean reward per episode: 31.83\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 73500\n",
      "Best mean reward: 31.83 - Last mean reward per episode: 32.46\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 73600\n",
      "Best mean reward: 32.46 - Last mean reward per episode: 33.39\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.2     |\n",
      "|    ep_rew_mean      | 33.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3508     |\n",
      "|    fps              | 2677     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 73691    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0315   |\n",
      "|    n_updates        | 5922     |\n",
      "----------------------------------\n",
      "Num timesteps: 73700\n",
      "Best mean reward: 33.39 - Last mean reward per episode: 33.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 32.8     |\n",
      "|    ep_rew_mean      | 32.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3512     |\n",
      "|    fps              | 2672     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 73790    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 5947     |\n",
      "----------------------------------\n",
      "Num timesteps: 73800\n",
      "Best mean reward: 33.39 - Last mean reward per episode: 32.80\n",
      "Num timesteps: 73900\n",
      "Best mean reward: 33.39 - Last mean reward per episode: 33.04\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.1     |\n",
      "|    ep_rew_mean      | 33.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3516     |\n",
      "|    fps              | 2665     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 73905    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0547   |\n",
      "|    n_updates        | 5976     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 74000\n",
      "Best mean reward: 33.39 - Last mean reward per episode: 33.47\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.4     |\n",
      "|    ep_rew_mean      | 33.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3520     |\n",
      "|    fps              | 2656     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 74030    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0384   |\n",
      "|    n_updates        | 6007     |\n",
      "----------------------------------\n",
      "Num timesteps: 74100\n",
      "Best mean reward: 33.47 - Last mean reward per episode: 33.03\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33       |\n",
      "|    ep_rew_mean      | 33       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3524     |\n",
      "|    fps              | 2648     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 74100    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00976  |\n",
      "|    n_updates        | 6024     |\n",
      "----------------------------------\n",
      "Num timesteps: 74200\n",
      "Best mean reward: 33.47 - Last mean reward per episode: 33.04\n",
      "Num timesteps: 74300\n",
      "Best mean reward: 33.47 - Last mean reward per episode: 33.88\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 34.9     |\n",
      "|    ep_rew_mean      | 34.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3528     |\n",
      "|    fps              | 2619     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74370    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0409   |\n",
      "|    n_updates        | 6092     |\n",
      "----------------------------------\n",
      "Num timesteps: 74400\n",
      "Best mean reward: 33.88 - Last mean reward per episode: 34.85\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 74500\n",
      "Best mean reward: 34.85 - Last mean reward per episode: 35.50\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.1     |\n",
      "|    ep_rew_mean      | 36.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3532     |\n",
      "|    fps              | 2606     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74568    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0148   |\n",
      "|    n_updates        | 6141     |\n",
      "----------------------------------\n",
      "Num timesteps: 74600\n",
      "Best mean reward: 35.50 - Last mean reward per episode: 36.14\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 74700\n",
      "Best mean reward: 36.14 - Last mean reward per episode: 36.35\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.9     |\n",
      "|    ep_rew_mean      | 36.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3536     |\n",
      "|    fps              | 2595     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.151    |\n",
      "|    n_updates        | 6189     |\n",
      "----------------------------------\n",
      "Num timesteps: 74800\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 35.51\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35       |\n",
      "|    ep_rew_mean      | 35       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3540     |\n",
      "|    fps              | 2592     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74817    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0534   |\n",
      "|    n_updates        | 6204     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 34.2     |\n",
      "|    ep_rew_mean      | 34.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3544     |\n",
      "|    fps              | 2590     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74866    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00414  |\n",
      "|    n_updates        | 6216     |\n",
      "----------------------------------\n",
      "Num timesteps: 74900\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 33.98\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.6     |\n",
      "|    ep_rew_mean      | 33.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3548     |\n",
      "|    fps              | 2586     |\n",
      "|    time_elapsed     | 28       |\n",
      "|    total_timesteps  | 74926    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0905   |\n",
      "|    n_updates        | 6231     |\n",
      "----------------------------------\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 33.48\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 33.6     |\n",
      "|    ep_rew_mean      | 33.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3552     |\n",
      "|    fps              | 2582     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 75011    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.114    |\n",
      "|    n_updates        | 6252     |\n",
      "----------------------------------\n",
      "Num timesteps: 75100\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 33.81\n",
      "Num timesteps: 75200\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 34.89\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35.6     |\n",
      "|    ep_rew_mean      | 35.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3556     |\n",
      "|    fps              | 2570     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 75283    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0963   |\n",
      "|    n_updates        | 6320     |\n",
      "----------------------------------\n",
      "Num timesteps: 75300\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 35.62\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 35.4     |\n",
      "|    ep_rew_mean      | 35.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3560     |\n",
      "|    fps              | 2565     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 75375    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0496   |\n",
      "|    n_updates        | 6343     |\n",
      "----------------------------------\n",
      "Num timesteps: 75400\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 35.40\n",
      "Num timesteps: 75500\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 34.84\n",
      "Num timesteps: 75600\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 36.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 36.7     |\n",
      "|    ep_rew_mean      | 36.7     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3564     |\n",
      "|    fps              | 2549     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 75689    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00183  |\n",
      "|    n_updates        | 6422     |\n",
      "----------------------------------\n",
      "Num timesteps: 75700\n",
      "Best mean reward: 36.35 - Last mean reward per episode: 36.71\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 75800\n",
      "Best mean reward: 36.71 - Last mean reward per episode: 37.35\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 37.4     |\n",
      "|    ep_rew_mean      | 37.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3568     |\n",
      "|    fps              | 2536     |\n",
      "|    time_elapsed     | 29       |\n",
      "|    total_timesteps  | 75879    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0335   |\n",
      "|    n_updates        | 6469     |\n",
      "----------------------------------\n",
      "Num timesteps: 75900\n",
      "Best mean reward: 37.35 - Last mean reward per episode: 37.36\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.09\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 38       |\n",
      "|    ep_rew_mean      | 38       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3572     |\n",
      "|    fps              | 2523     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 76089    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0757   |\n",
      "|    n_updates        | 6522     |\n",
      "----------------------------------\n",
      "Num timesteps: 76100\n",
      "Best mean reward: 37.36 - Last mean reward per episode: 37.95\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 76200\n",
      "Best mean reward: 37.95 - Last mean reward per episode: 38.36\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 76300\n",
      "Best mean reward: 38.36 - Last mean reward per episode: 39.27\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 76400\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 39.24\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 39.2     |\n",
      "|    ep_rew_mean      | 39.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3576     |\n",
      "|    fps              | 2500     |\n",
      "|    time_elapsed     | 30       |\n",
      "|    total_timesteps  | 76400    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0907   |\n",
      "|    n_updates        | 6599     |\n",
      "----------------------------------\n",
      "Num timesteps: 76500\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 39.01\n",
      "Num timesteps: 76600\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 39.17\n",
      "Num timesteps: 76700\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 39.17\n",
      "Num timesteps: 76800\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 39.17\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 41.8     |\n",
      "|    ep_rew_mean      | 41.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3580     |\n",
      "|    fps              | 2478     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 76887    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0913   |\n",
      "|    n_updates        | 6721     |\n",
      "----------------------------------\n",
      "Num timesteps: 76900\n",
      "Best mean reward: 39.27 - Last mean reward per episode: 41.75\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 41.75 - Last mean reward per episode: 42.04\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77100\n",
      "Best mean reward: 42.04 - Last mean reward per episode: 42.74\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77200\n",
      "Best mean reward: 42.74 - Last mean reward per episode: 42.74\n",
      "Num timesteps: 77300\n",
      "Best mean reward: 42.74 - Last mean reward per episode: 42.74\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 45.6     |\n",
      "|    ep_rew_mean      | 45.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3584     |\n",
      "|    fps              | 2442     |\n",
      "|    time_elapsed     | 31       |\n",
      "|    total_timesteps  | 77352    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.022    |\n",
      "|    n_updates        | 6837     |\n",
      "----------------------------------\n",
      "Num timesteps: 77400\n",
      "Best mean reward: 42.74 - Last mean reward per episode: 45.59\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77500\n",
      "Best mean reward: 45.59 - Last mean reward per episode: 46.65\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77600\n",
      "Best mean reward: 46.65 - Last mean reward per episode: 47.63\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 47.9     |\n",
      "|    ep_rew_mean      | 47.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3588     |\n",
      "|    fps              | 2423     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 77669    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.165    |\n",
      "|    n_updates        | 6917     |\n",
      "----------------------------------\n",
      "Num timesteps: 77700\n",
      "Best mean reward: 47.63 - Last mean reward per episode: 47.82\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77800\n",
      "Best mean reward: 47.82 - Last mean reward per episode: 48.11\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 77900\n",
      "Best mean reward: 48.11 - Last mean reward per episode: 49.22\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 49.6     |\n",
      "|    ep_rew_mean      | 49.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3592     |\n",
      "|    fps              | 2409     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 77942    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 6985     |\n",
      "----------------------------------\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 49.22 - Last mean reward per episode: 49.73\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 78100\n",
      "Best mean reward: 49.73 - Last mean reward per episode: 49.87\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 50.8     |\n",
      "|    ep_rew_mean      | 50.8     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3596     |\n",
      "|    fps              | 2400     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 78139    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00929  |\n",
      "|    n_updates        | 7034     |\n",
      "----------------------------------\n",
      "Num timesteps: 78200\n",
      "Best mean reward: 49.87 - Last mean reward per episode: 50.81\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 78300\n",
      "Best mean reward: 50.81 - Last mean reward per episode: 52.18\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 78400\n",
      "Best mean reward: 52.18 - Last mean reward per episode: 52.83\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 53       |\n",
      "|    ep_rew_mean      | 53       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3600     |\n",
      "|    fps              | 2387     |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 78446    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0394   |\n",
      "|    n_updates        | 7111     |\n",
      "----------------------------------\n",
      "Num timesteps: 78500\n",
      "Best mean reward: 52.83 - Last mean reward per episode: 53.01\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 78600\n",
      "Best mean reward: 53.01 - Last mean reward per episode: 53.01\n",
      "Num timesteps: 78700\n",
      "Best mean reward: 53.01 - Last mean reward per episode: 53.01\n",
      "Num timesteps: 78800\n",
      "Best mean reward: 53.01 - Last mean reward per episode: 54.64\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 78900\n",
      "Best mean reward: 54.64 - Last mean reward per episode: 54.64\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 56.1     |\n",
      "|    ep_rew_mean      | 56.1     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3604     |\n",
      "|    fps              | 2367     |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 78987    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00524  |\n",
      "|    n_updates        | 7246     |\n",
      "----------------------------------\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 54.64 - Last mean reward per episode: 56.06\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 79100\n",
      "Best mean reward: 56.06 - Last mean reward per episode: 56.06\n",
      "Num timesteps: 79200\n",
      "Best mean reward: 56.06 - Last mean reward per episode: 56.06\n",
      "Num timesteps: 79300\n",
      "Best mean reward: 56.06 - Last mean reward per episode: 58.19\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 79400\n",
      "Best mean reward: 58.19 - Last mean reward per episode: 57.02\n",
      "Num timesteps: 79500\n",
      "Best mean reward: 58.19 - Last mean reward per episode: 57.02\n",
      "Num timesteps: 79600\n",
      "Best mean reward: 58.19 - Last mean reward per episode: 57.02\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 59.2     |\n",
      "|    ep_rew_mean      | 59.2     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3608     |\n",
      "|    fps              | 2340     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 79610    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00212  |\n",
      "|    n_updates        | 7402     |\n",
      "----------------------------------\n",
      "Num timesteps: 79700\n",
      "Best mean reward: 58.19 - Last mean reward per episode: 59.19\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 79800\n",
      "Best mean reward: 59.19 - Last mean reward per episode: 59.19\n",
      "Num timesteps: 79900\n",
      "Best mean reward: 59.19 - Last mean reward per episode: 61.78\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 61.78 - Last mean reward per episode: 61.78\n",
      "Num timesteps: 80100\n",
      "Best mean reward: 61.78 - Last mean reward per episode: 63.39\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 63.4     |\n",
      "|    ep_rew_mean      | 63.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3612     |\n",
      "|    fps              | 2321     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 80125    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0653   |\n",
      "|    n_updates        | 7531     |\n",
      "----------------------------------\n",
      "Num timesteps: 80200\n",
      "Best mean reward: 63.39 - Last mean reward per episode: 63.24\n",
      "Num timesteps: 80300\n",
      "Best mean reward: 63.39 - Last mean reward per episode: 63.89\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 64.5     |\n",
      "|    ep_rew_mean      | 64.5     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3616     |\n",
      "|    fps              | 2313     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total_timesteps  | 80356    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0357   |\n",
      "|    n_updates        | 7588     |\n",
      "----------------------------------\n",
      "Num timesteps: 80400\n",
      "Best mean reward: 63.89 - Last mean reward per episode: 64.51\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 80500\n",
      "Best mean reward: 64.51 - Last mean reward per episode: 64.51\n",
      "Num timesteps: 80600\n",
      "Best mean reward: 64.51 - Last mean reward per episode: 65.60\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 80700\n",
      "Best mean reward: 65.60 - Last mean reward per episode: 65.60\n",
      "Num timesteps: 80800\n",
      "Best mean reward: 65.60 - Last mean reward per episode: 67.92\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 80900\n",
      "Best mean reward: 67.92 - Last mean reward per episode: 68.69\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 68.69 - Last mean reward per episode: 68.69\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.4     |\n",
      "|    ep_rew_mean      | 70.4     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3620     |\n",
      "|    fps              | 2287     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 81073    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0976   |\n",
      "|    n_updates        | 7768     |\n",
      "----------------------------------\n",
      "Num timesteps: 81100\n",
      "Best mean reward: 68.69 - Last mean reward per episode: 70.43\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70.9     |\n",
      "|    ep_rew_mean      | 70.9     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3624     |\n",
      "|    fps              | 2283     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 81187    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0506   |\n",
      "|    n_updates        | 7796     |\n",
      "----------------------------------\n",
      "Num timesteps: 81200\n",
      "Best mean reward: 70.43 - Last mean reward per episode: 70.87\n",
      "Saving new best model to /tmp/gym/DQN/CartPole/best_model\n",
      "Num timesteps: 81300\n",
      "Best mean reward: 70.87 - Last mean reward per episode: 70.27\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 70       |\n",
      "|    ep_rew_mean      | 70       |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 3628     |\n",
      "|    fps              | 2276     |\n",
      "|    time_elapsed     | 35       |\n",
      "|    total_timesteps  | 81374    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0665   |\n",
      "|    n_updates        | 7843     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from logging_utils import SaveOnBestTrainingRewardCallback\n",
    "\n",
    "log_dir = \"/tmp/gym/DQN/CartPole\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", # name of your gym environement  \n",
    "               render_mode=None,#\"human\"\n",
    "              )\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir)\n",
    "model = DQN(\"MlpPolicy\", # type de réseaux de neuronnes \n",
    "            env, # passe l'environement\n",
    "            verbose=1, # mode verbeux ( affiche plus d'information )\n",
    "           )\n",
    "trained_model = model.learn(\n",
    "    total_timesteps=100000, # temps d'entrainement\n",
    "    log_interval=4, # intervalle de remonter des données\n",
    "    callback=callback, # Utiliser pour stocker des métriques\n",
    ") \n",
    "# renvoie un modèle entrainé\n",
    "model.save(\"dqn_cartpole\") # sauvegarde le modèle entrainé dans un fichier\n",
    "model = DQN.load(\"dqn_cartpole\") # charge le fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbf899",
   "metadata": {},
   "source": [
    "### Afficher la récompense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7121b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Helper from the library\n",
    "results_plotter.plot_results(\n",
    "    [log_dir], 1e5, results_plotter.X_TIMESTEPS, \"CartPole V1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_utils import plot_results\n",
    "\n",
    "\n",
    "plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592cd19",
   "metadata": {},
   "source": [
    "## Teste de l'agent \n",
    "\n",
    "Une fois l'agent entrainé il faut le tester et vérifier que le comportement obtenu est le bon ou proche du bon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset() # réinitialisation de l'environement \n",
    "# obs corresponds à l'observation \n",
    "# info contiens des information aditionelle\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)  # le réseaux de neuronnes choisie une action\n",
    "    obs, reward, terminated, truncated, info = env.step(action) # execute l'environemenet d'un step / d'un tour d'horloge\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba59c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
